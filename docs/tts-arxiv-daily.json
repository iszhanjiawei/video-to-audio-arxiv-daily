{"video-to-speech Papers": {"2601.18633": "|**2026-01-26**|**Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting**|Tong Shi et.al.|[2601.18633](http://arxiv.org/abs/2601.18633)|null|\n", "2601.17645": "|**2026-01-25**|**AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking**|Xilin Jiang et.al.|[2601.17645](http://arxiv.org/abs/2601.17645)|null|\n", "2509.13145": "|**2025-10-31**|**UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System Based on Multimodal Large Language Model**|Yudong Yang et.al.|[2509.13145](http://arxiv.org/abs/2509.13145)|null|\n", "2508.02376": "|**2025-08-04**|**Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction**|Matus Krajcovic et.al.|[2508.02376](http://arxiv.org/abs/2508.02376)|null|\n", "2506.04134": "|**2025-11-11**|**UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation**|Jinting Wang et.al.|[2506.04134](http://arxiv.org/abs/2506.04134)|null|\n", "2504.20629": "|**2025-10-03**|**AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation**|Jeongsoo Choi et.al.|[2504.20629](http://arxiv.org/abs/2504.20629)|null|\n", "2504.02386": "|**2025-04-03**|**VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models**|Kim Sung-Bin et.al.|[2504.02386](http://arxiv.org/abs/2504.02386)|null|\n", "2503.23307": "|**2025-03-30**|**MoCha: Towards Movie-Grade Talking Character Synthesis**|Cong Wei et.al.|[2503.23307](http://arxiv.org/abs/2503.23307)|null|\n", "2503.22668": "|**2025-08-21**|**Understanding Co-speech Gestures in-the-wild**|Sindhu B Hegde et.al.|[2503.22668](http://arxiv.org/abs/2503.22668)|null|\n", "2503.22265": "|**2025-03-28**|**DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech and Audio Generation**|Haomin Zhang et.al.|[2503.22265](http://arxiv.org/abs/2503.22265)|null|\n", "2503.16956": "|**2025-03-21**|**From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech**|Ji-Hoon Kim et.al.|[2503.16956](http://arxiv.org/abs/2503.16956)|null|\n", "2503.14928": "|**2025-03-19**|**Shushing! Let's Imagine an Authentic Speech from the Silent Video**|Jiaxin Ye et.al.|[2503.14928](http://arxiv.org/abs/2503.14928)|null|\n", "2503.08714": "|**2025-07-27**|**Versatile Multimodal Controls for Expressive Talking Human Animation**|Zheng Qin et.al.|[2503.08714](http://arxiv.org/abs/2503.08714)|null|\n", "2503.05223": "|**2025-03-07**|**DiVISe: Direct Visual-Input Speech Synthesis Preserving Speaker Characteristics And Intelligibility**|Yifan Liu et.al.|[2503.05223](http://arxiv.org/abs/2503.05223)|null|\n", "2502.07538": "|**2025-02-13**|**Visual-based spatial audio generation system for multi-speaker environments**|Xiaojing Liu et.al.|[2502.07538](http://arxiv.org/abs/2502.07538)|null|\n", "2501.01957": "|**2025-10-24**|**VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**|Chaoyou Fu et.al.|[2501.01957](http://arxiv.org/abs/2501.01957)|null|\n", "2412.06264": "|**2024-12-09**|**Flow Matching Guide and Code**|Yaron Lipman et.al.|[2412.06264](http://arxiv.org/abs/2412.06264)|null|\n", "2411.19486": "|**2025-05-30**|**V2SFlow: Video-to-Speech Generation with Speech Decomposition and Rectified Flow**|Jeongsoo Choi et.al.|[2411.19486](http://arxiv.org/abs/2411.19486)|null|\n", "2411.17690": "|**2025-05-29**|**Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis**|Akshita Gupta et.al.|[2411.17690](http://arxiv.org/abs/2411.17690)|null|\n", "2411.15457": "|**2024-11-23**|**Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset**|Sukhandeep Kaur et.al.|[2411.15457](http://arxiv.org/abs/2411.15457)|null|\n", "2411.10510": "|**2025-05-21**|**SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers**|Joseph Liu et.al.|[2411.10510](http://arxiv.org/abs/2411.10510)|null|\n", "2409.10710": "|**2024-09-16**|**A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children**|Isabella Pu et.al.|[2409.10710](http://arxiv.org/abs/2409.10710)|null|\n", "2408.04535": "|**2024-08-11**|**Synchronous Multi-modal Semantic Communication System with Packet-level Coding**|Yun Tian et.al.|[2408.04535](http://arxiv.org/abs/2408.04535)|null|\n", "2407.21783": "|**2024-11-23**|**The Llama 3 Herd of Models**|Aaron Grattafiori et.al.|[2407.21783](http://arxiv.org/abs/2407.21783)|null|\n", "2407.09147": "|**2024-07-12**|**AI-Powered Immersive Assistance for Interactive Task Execution in Industrial Environments**|Tomislav Duricic et.al.|[2407.09147](http://arxiv.org/abs/2407.09147)|null|\n", "2406.15704": "|**2024-06-22**|**video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models**|Guangzhi Sun et.al.|[2406.15704](http://arxiv.org/abs/2406.15704)|null|\n", "2406.08802": "|**2024-06-13**|**DubWise: Video-Guided Speech Duration Control in Multimodal LLM-based Text-to-Speech for Dubbing**|Neha Sahipjohn et.al.|[2406.08802](http://arxiv.org/abs/2406.08802)|null|\n", "2404.16112": "|**2024-04-24**|**Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges**|Badri Narayana Patro et.al.|[2404.16112](http://arxiv.org/abs/2404.16112)|null|\n", "2403.15377": "|**2024-08-14**|**InternVideo2: Scaling Foundation Models for Multimodal Video Understanding**|Yi Wang et.al.|[2403.15377](http://arxiv.org/abs/2403.15377)|null|\n", "2312.11512": "|**2023-12-12**|**Path Signature Representation of Patient-Clinician Interactions as a Predictor for Neuropsychological Tests Outcomes in Children: A Proof of Concept**|Giulio Falcioni et.al.|[2312.11512](http://arxiv.org/abs/2312.11512)|null|\n", "2311.17655": "|**2023-11-29**|**Vulnerability of Automatic Identity Recognition to Audio-Visual Deepfakes**|Pavel Korshunov et.al.|[2311.17655](http://arxiv.org/abs/2311.17655)|null|\n", "2311.15230": "|**2024-03-14**|**GAIA: Zero-shot Talking Avatar Generation**|Tianyu He et.al.|[2311.15230](http://arxiv.org/abs/2311.15230)|null|\n", "2308.12610": "|**2024-12-08**|**Emotion-Aligned Contrastive Learning Between Images and Music**|Shanti Stewart et.al.|[2308.12610](http://arxiv.org/abs/2308.12610)|null|\n", "2308.07787": "|**2023-08-15**|**DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding**|Jeongsoo Choi et.al.|[2308.07787](http://arxiv.org/abs/2308.07787)|null|\n", "2308.06246": "|**2023-08-11**|**ARGUS: Visualization of AI-Assisted Task Guidance in AR**|Sonia Castelo et.al.|[2308.06246](http://arxiv.org/abs/2308.06246)|null|\n", "2307.16584": "|**2023-07-31**|**Audio-visual video-to-speech synthesis with synthesized input audio**|Triantafyllos Kefalas et.al.|[2307.16584](http://arxiv.org/abs/2307.16584)|null|\n", "2307.08581": "|**2023-07-17**|**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**|Yang Zhao et.al.|[2307.08581](http://arxiv.org/abs/2307.08581)|null|\n", "2306.15464": "|**2023-07-31**|**Large-scale unsupervised audio pre-training for video-to-speech synthesis**|Triantafyllos Kefalas et.al.|[2306.15464](http://arxiv.org/abs/2306.15464)|null|\n", "2303.12423": "|**2023-03-25**|**Text with Knowledge Graph Augmented Transformer for Video Captioning**|Xin Gu et.al.|[2303.12423](http://arxiv.org/abs/2303.12423)|null|\n", "2212.11377": "|**2022-12-21**|**ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement**|Wei-Ning Hsu et.al.|[2212.11377](http://arxiv.org/abs/2212.11377)|null|\n", "2210.15876": "|**2023-05-25**|**Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition**|Yist Y. Lin et.al.|[2210.15876](http://arxiv.org/abs/2210.15876)|null|\n", "2206.12127": "|**2022-06-24**|**Implicit Channel Learning for Machine Learning Applications in 6G Wireless Networks**|Ahmet M. Elbir et.al.|[2206.12127](http://arxiv.org/abs/2206.12127)|null|\n", "2206.07458": "|**2022-07-20**|**VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection**|Joanna Hong et.al.|[2206.07458](http://arxiv.org/abs/2206.07458)|null|\n", "2205.02058": "|**2022-08-15**|**SVTS: Scalable Video-to-Speech Synthesis**|Rodrigo Mira et.al.|[2205.02058](http://arxiv.org/abs/2205.02058)|null|\n", "2204.11030": "|**2022-04-23**|**Improving Self-Supervised Learning-based MOS Prediction Networks**|B\u00e1lint Gyires-T\u00f3th et.al.|[2204.11030](http://arxiv.org/abs/2204.11030)|null|\n", "2203.06004": "|**2022-03-09**|**An error correction scheme for improved air-tissue boundary in real-time MRI video for speech production**|Anwesha Roy et.al.|[2203.06004](http://arxiv.org/abs/2203.06004)|null|\n", "2203.03601": "|**2022-03-07**|**Creating Speech-to-Speech Corpus from Dubbed Series**|Massa Baali et.al.|[2203.03601](http://arxiv.org/abs/2203.03601)|null|\n", "2202.09695": "|**2022-04-25**|**SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their Descriptions**|Viet Dac Lai et.al.|[2202.09695](http://arxiv.org/abs/2202.09695)|null|\n", "2202.09081": "|**2022-02-18**|**VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion**|Disong Wang et.al.|[2202.09081](http://arxiv.org/abs/2202.09081)|null|\n", "2201.04736": "|**2023-12-17**|**Security for Machine Learning-based Software Systems: a survey of threats, practices and challenges**|Huaming Chen et.al.|[2201.04736](http://arxiv.org/abs/2201.04736)|null|\n", "2107.05548": "|**2022-03-04**|**Multi-modality Deep Restoration of Extremely Compressed Face Videos**|Xi Zhang et.al.|[2107.05548](http://arxiv.org/abs/2107.05548)|null|\n", "2105.09652": "|**2021-05-20**|**Speaker disentanglement in video-to-speech conversion**|Dan Oneata et.al.|[2105.09652](http://arxiv.org/abs/2105.09652)|null|\n", "2104.13332": "|**2022-08-15**|**End-to-End Video-To-Speech Synthesis using Generative Adversarial Networks**|Rodrigo Mira et.al.|[2104.13332](http://arxiv.org/abs/2104.13332)|null|\n", "2104.10217": "|**2021-04-20**|**Bias-Aware Loss for Training Image and Speech Quality Prediction Models from Multiple Datasets**|Gabriel Mittag et.al.|[2104.10217](http://arxiv.org/abs/2104.10217)|null|\n", "2102.09281": "|**2021-02-18**|**DINO: A Conditional Energy-Based GAN for Domain Translation**|Konstantinos Vougioukas et.al.|[2102.09281](http://arxiv.org/abs/2102.09281)|null|\n", "2012.15391": "|**2020-12-31**|**Generalized Operating Procedure for Deep Learning: an Unconstrained Optimal Design Perspective**|Shen Chen et.al.|[2012.15391](http://arxiv.org/abs/2012.15391)|null|\n", "2006.09888": "|**2020-10-22**|**Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation of Facial Gestures in Dyadic Settings**|Patrik Jonell et.al.|[2006.09888](http://arxiv.org/abs/2006.09888)|null|\n", "2005.07074": "|**2020-05-14**|**FaceFilter: Audio-visual speech separation using still images**|Soo-Whan Chung et.al.|[2005.07074](http://arxiv.org/abs/2005.07074)|null|\n", "2004.02541": "|**2020-08-15**|**Vocoder-Based Speech Synthesis from Silent Videos**|Daniel Michelsanti et.al.|[2004.02541](http://arxiv.org/abs/2004.02541)|null|\n", "1906.06301": "|**2019-06-14**|**Video-Driven Speech Reconstruction using Generative Adversarial Networks**|Konstantinos Vougioukas et.al.|[1906.06301](http://arxiv.org/abs/1906.06301)|null|\n", "1901.10584": "|**2019-01-29**|**Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A Co-Design Approach**|Nitthilan Kannappan Jayakodi et.al.|[1901.10584](http://arxiv.org/abs/1901.10584)|null|\n", "1901.08623": "|**2019-02-01**|**Petrophysical Property Estimation from Seismic Data Using Recurrent Neural Networks**|Motaz Alfarraj et.al.|[1901.08623](http://arxiv.org/abs/1901.08623)|null|\n", "1810.08403": "|**2018-10-19**|**Towards Efficient Large-Scale Graph Neural Network Computing**|Lingxiao Ma et.al.|[1810.08403](http://arxiv.org/abs/1810.08403)|null|\n", "1804.03619": "|**2018-08-09**|**Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation**|Ariel Ephrat et.al.|[1804.03619](http://arxiv.org/abs/1804.03619)|null|\n", "1711.00937": "|**2018-05-30**|**Neural Discrete Representation Learning**|Aaron van den Oord et.al.|[1711.00937](http://arxiv.org/abs/1711.00937)|null|\n", "1708.06767": "|**2018-02-09**|**Seeing Through Noise: Visually Driven Speaker Separation and Enhancement**|Aviv Gabbay et.al.|[1708.06767](http://arxiv.org/abs/1708.06767)|null|\n", "1707.01886": "|**2018-03-12**|**Buildup of Speaking Skills in an Online Learning Community: A Network-Analytic Exploration**|Rasoul Shafipour et.al.|[1707.01886](http://arxiv.org/abs/1707.01886)|null|\n", "1705.02966": "|**2017-07-18**|**You said that?**|Joon Son Chung et.al.|[1705.02966](http://arxiv.org/abs/1705.02966)|null|\n", "1702.07486": "|**2017-04-13**|**Deep representation learning for human motion prediction and classification**|Judith B\u00fctepage et.al.|[1702.07486](http://arxiv.org/abs/1702.07486)|null|\n", "1606.09375": "|**2017-02-05**|**Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering**|Micha\u00ebl Defferrard et.al.|[1606.09375](http://arxiv.org/abs/1606.09375)|null|\n", "1607.01039": "|**2016-12-24**|**Practical Tera-scale Walsh-Hadamard Transform**|Yi Lu et.al.|[1607.01039](http://arxiv.org/abs/1607.01039)|null|\n", "math/0703438": "|**2007-03-14**|**Wavelets on Irregular Grids with Arbitrary Dilation Matrices, and Frames Atoms for L^2(R^d)**|Akram Aldroubi et.al.|[math/0703438](http://arxiv.org/abs/math/0703438)|null|\n"}, "video-to-audio Papers": {"2602.02204": "|**2026-02-02**|**vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models**|Peiqi Yin et.al.|[2602.02204](http://arxiv.org/abs/2602.02204)|null|\n", "2602.00846": "|**2026-01-31**|**Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis**|Zicheng Kong et.al.|[2602.00846](http://arxiv.org/abs/2602.00846)|null|\n", "2601.19606": "|**2026-01-27**|**GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining**|Shentong Mo et.al.|[2601.19606](http://arxiv.org/abs/2601.19606)|null|\n", "2601.17530": "|**2026-01-24**|**Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes**|Gautam Siddharth Kashyap et.al.|[2601.17530](http://arxiv.org/abs/2601.17530)|null|\n", "2601.16870": "|**2026-01-23**|**A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study**|Guangping Liu et.al.|[2601.16870](http://arxiv.org/abs/2601.16870)|null|\n", "2601.15278": "|**2026-01-21**|**Interpreting Multimodal Communication at Scale in Short-Form Video: Visual, Audio, and Textual Mental Health Discourse on TikTok**|Mingyue Zha et.al.|[2601.15278](http://arxiv.org/abs/2601.15278)|null|\n", "2601.15017": "|**2026-01-29**|**SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation**|Yanan Wang et.al.|[2601.15017](http://arxiv.org/abs/2601.15017)|null|\n", "2601.12844": "|**2026-01-19**|**Rapport du Projet de Recherche TRAIMA**|Julie Ran\u00e7on et.al.|[2601.12844](http://arxiv.org/abs/2601.12844)|null|\n", "2601.06870": "|**2026-01-11**|**DaQ-MSA: Denoising and Qualifying Diffusion Augmentations for Multimodal Sentiment Analysis**|Jiazhang Liang et.al.|[2601.06870](http://arxiv.org/abs/2601.06870)|null|\n", "2601.06573": "|**2026-01-10**|**QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models**|Zixing Lin et.al.|[2601.06573](http://arxiv.org/abs/2601.06573)|null|\n", "2601.05059": "|**2026-01-08**|**From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)**|Suyash Mishra et.al.|[2601.05059](http://arxiv.org/abs/2601.05059)|null|\n", "2601.02731": "|**2026-01-11**|**Omni2Sound: Towards Unified Video-Text-to-Audio Generation**|Yusheng Dai et.al.|[2601.02731](http://arxiv.org/abs/2601.02731)|null|\n", "2512.21094": "|**2025-12-24**|**T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation**|Zhe Cao et.al.|[2512.21094](http://arxiv.org/abs/2512.21094)|null|\n", "2512.13677": "|**2025-12-15**|**JoVA: Unified Multimodal Learning for Joint Video-Audio Generation**|Xiaohu Huang et.al.|[2512.13677](http://arxiv.org/abs/2512.13677)|null|\n", "2512.13281": "|**2025-12-18**|**Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?**|Jiaqi Wang et.al.|[2512.13281](http://arxiv.org/abs/2512.13281)|null|\n", "2512.10652": "|**2025-12-23**|**TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection**|Jian-Yu Jiang-Lin et.al.|[2512.10652](http://arxiv.org/abs/2512.10652)|null|\n", "2512.09841": "|**2025-12-10**|**ChronusOmni: Improving Time Awareness of Omni Large Language Models**|Yijing Chen et.al.|[2512.09841](http://arxiv.org/abs/2512.09841)|null|\n", "2512.09299": "|**2025-12-10**|**VABench: A Comprehensive Benchmark for Audio-Video Generation**|Daili Hua et.al.|[2512.09299](http://arxiv.org/abs/2512.09299)|null|\n", "2512.08282": "|**2025-12-09**|**PAVAS: Physics-Aware Video-to-Audio Synthesis**|Oh Hyun-Bin et.al.|[2512.08282](http://arxiv.org/abs/2512.08282)|null|\n", "2512.07209": "|**2025-12-08**|**Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits**|Masato Ishii et.al.|[2512.07209](http://arxiv.org/abs/2512.07209)|null|\n", "2512.05745": "|**2025-12-05**|**ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior**|Weikai Lu et.al.|[2512.05745](http://arxiv.org/abs/2512.05745)|null|\n", "2512.06022": "|**2025-12-04**|**DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation**|Fu Li et.al.|[2512.06022](http://arxiv.org/abs/2512.06022)|null|\n", "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|Mengchen Zhang et.al.|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n", "2512.02650": "|**2025-12-02**|**Hear What Matters! Text-conditioned Selective Video-to-Audio Generation**|Junwon Lee et.al.|[2512.02650](http://arxiv.org/abs/2512.02650)|null|\n", "2512.02492": "|**2025-12-02**|**YingVideo-MV: Music-Driven Multi-Stage Video Generation**|Jiahui Chen et.al.|[2512.02492](http://arxiv.org/abs/2512.02492)|null|\n", "2512.02306": "|**2025-12-02**|**OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning**|Boyu Zhu et.al.|[2512.02306](http://arxiv.org/abs/2512.02306)|null|\n", "2511.22488": "|**2025-11-27**|**AI killed the video star. Audio-driven diffusion model for expressive talking head generation**|Baptiste Chopin et.al.|[2511.22488](http://arxiv.org/abs/2511.22488)|null|\n", "2511.22455": "|**2025-11-27**|**Beyond Real versus Fake Towards Intent-Aware Video Analysis**|Saurabh Atreya et.al.|[2511.22455](http://arxiv.org/abs/2511.22455)|null|\n", "2511.22443": "|**2025-11-27**|**Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition**|Maheswar Bora et.al.|[2511.22443](http://arxiv.org/abs/2511.22443)|null|\n", "2511.21579": "|**2025-11-28**|**Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy**|Teng Hu et.al.|[2511.21579](http://arxiv.org/abs/2511.21579)|null|\n", "2511.18833": "|**2025-11-27**|**PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation**|Huadai Liu et.al.|[2511.18833](http://arxiv.org/abs/2511.18833)|null|\n", "2511.18698": "|**2025-11-24**|**Multimodal Real-Time Anomaly Detection and Industrial Applications**|Aman Verma et.al.|[2511.18698](http://arxiv.org/abs/2511.18698)|null|\n", "2511.20686": "|**2025-11-20**|**AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI**|Chae-Gyun Lim et.al.|[2511.20686](http://arxiv.org/abs/2511.20686)|null|\n", "2511.15312": "|**2025-11-19**|**A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data**|Mauro Larrat et.al.|[2511.15312](http://arxiv.org/abs/2511.15312)|null|\n", "2511.13219": "|**2025-11-24**|**FoleyBench: A Benchmark For Video-to-Audio Models**|Satvik Dixit et.al.|[2511.13219](http://arxiv.org/abs/2511.13219)|null|\n", "2511.11106": "|**2025-11-14**|**AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization**|Zhonghua Jiang et.al.|[2511.11106](http://arxiv.org/abs/2511.11106)|null|\n", "2511.03489": "|**2025-11-05**|**Analytical Queries for Unstructured Data**|Daniel Kang et.al.|[2511.03489](http://arxiv.org/abs/2511.03489)|null|\n", "2511.03334": "|**2025-11-05**|**UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions**|Guozhen Zhang et.al.|[2511.03334](http://arxiv.org/abs/2511.03334)|null|\n", "2511.02371": "|**2025-11-04**|**LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment**|Rohan Wandre et.al.|[2511.02371](http://arxiv.org/abs/2511.02371)|null|\n", "2511.00103": "|**2025-10-30**|**FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video**|Rotem Ezra et.al.|[2511.00103](http://arxiv.org/abs/2511.00103)|null|\n", "2510.24103": "|**2025-10-28**|**Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation**|Kang Zhang et.al.|[2510.24103](http://arxiv.org/abs/2510.24103)|null|\n", "2510.18075": "|**2025-10-20**|**Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods**|Justus Arweiler et.al.|[2510.18075](http://arxiv.org/abs/2510.18075)|null|\n", "2510.18014": "|**2025-10-20**|**ManzaiSet: A Multimodal Dataset of Viewer Responses to Japanese Manzai Comedy**|Kazuki Kawamura et.al.|[2510.18014](http://arxiv.org/abs/2510.18014)|null|\n", "2510.13721": "|**2025-10-16**|**NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching**|Run Luo et.al.|[2510.13721](http://arxiv.org/abs/2510.13721)|null|\n", "2510.11579": "|**2025-10-13**|**MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis**|Hongyu Zhu et.al.|[2510.11579](http://arxiv.org/abs/2510.11579)|null|\n", "2510.09065": "|**2025-10-10**|**MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation**|Akira Takahashi et.al.|[2510.09065](http://arxiv.org/abs/2510.09065)|null|\n", "2510.08078": "|**2025-12-25**|**Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation**|Liyang Chen et.al.|[2510.08078](http://arxiv.org/abs/2510.08078)|null|\n", "2510.07837": "|**2025-10-09**|**IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries**|Harsh Kavediya et.al.|[2510.07837](http://arxiv.org/abs/2510.07837)|null|\n", "2510.06512": "|**2025-10-07**|**LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval**|Avishree Khare et.al.|[2510.06512](http://arxiv.org/abs/2510.06512)|null|\n", "2510.06060": "|**2025-10-07**|**Controllable Audio-Visual Viewpoint Generation from 360\u00b0 Spatial Information**|Christian Marinoni et.al.|[2510.06060](http://arxiv.org/abs/2510.06060)|null|\n", "2510.05829": "|**2025-10-07**|**FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders**|Riccardo Fosco Gramaccioni et.al.|[2510.05829](http://arxiv.org/abs/2510.05829)|null|\n", "2510.05828": "|**2025-10-07**|**StereoSync: Spatially-Aware Stereo Audio Generation from Video**|Christian Marinoni et.al.|[2510.05828](http://arxiv.org/abs/2510.05828)|null|\n", "2510.05717": "|**2025-10-07**|**DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities**|Hedi Zisling et.al.|[2510.05717](http://arxiv.org/abs/2510.05717)|null|\n", "2510.03906": "|**2026-01-26**|**From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance**|Ardalan Aryashad et.al.|[2510.03906](http://arxiv.org/abs/2510.03906)|null|\n", "2510.02916": "|**2025-10-03**|**SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos**|Amir Dellali et.al.|[2510.02916](http://arxiv.org/abs/2510.02916)|null|\n", "2510.02617": "|**2025-10-02**|**Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation**|Beijia Lu et.al.|[2510.02617](http://arxiv.org/abs/2510.02617)|null|\n", "2510.06235": "|**2025-10-02**|**Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)**|Robert Scholz et.al.|[2510.06235](http://arxiv.org/abs/2510.06235)|null|\n", "2510.02110": "|**2025-10-02**|**SoundReactor: Frame-level Online Video-to-Audio Generation**|Koichi Saito et.al.|[2510.02110](http://arxiv.org/abs/2510.02110)|null|\n", "2510.01284": "|**2025-09-30**|**Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation**|Chetwin Low et.al.|[2510.01284](http://arxiv.org/abs/2510.01284)|null|\n", "2509.24550": "|**2025-09-29**|**Training-Free Multimodal Guidance for Video to Audio Generation**|Eleonora Grassucci et.al.|[2509.24550](http://arxiv.org/abs/2509.24550)|null|\n", "2509.23727": "|**2025-09-28**|**AudioMoG: Guiding Audio Generation with Mixture-of-Guidance**|Junyou Wang et.al.|[2509.23727](http://arxiv.org/abs/2509.23727)|null|\n", "2509.21990": "|**2025-09-26**|**WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM**|Changli Tang et.al.|[2509.21990](http://arxiv.org/abs/2509.21990)|null|\n", "2509.21893": "|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|Jibin Song et.al.|[2509.21893](http://arxiv.org/abs/2509.21893)|null|\n", "2509.19999": "|**2025-11-04**|**MultiSoundGen: Video-to-Audio Generation for Multi-Event Scenarios via SlowFast Contrastive Audio-Visual Pretraining and Direct Preference Optimization**|Jianxuan Yang et.al.|[2509.19999](http://arxiv.org/abs/2509.19999)|null|\n", "2509.18272": "|**2025-10-05**|**StereoFoley: Object-Aware Stereo Audio Generation from Video**|Tornike Karchkhadze et.al.|[2509.18272](http://arxiv.org/abs/2509.18272)|null|\n", "2509.17022": "|**2025-09-21**|**VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module**|Kam Man Wu et.al.|[2509.17022](http://arxiv.org/abs/2509.17022)|null|\n", "2509.15492": "|**2025-09-19**|**Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech**|Xinlei Niu et.al.|[2509.15492](http://arxiv.org/abs/2509.15492)|null|\n", "2509.14479": "|**2025-09-17**|**A long-form single-speaker real-time MRI speech dataset and benchmark**|Sean Foley et.al.|[2509.14479](http://arxiv.org/abs/2509.14479)|null|\n", "2509.12052": "|**2025-10-16**|**AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided Autoregressive Perspective**|Yuchen Deng et.al.|[2509.12052](http://arxiv.org/abs/2509.12052)|null|\n", "2509.09940": "|**2025-09-12**|**DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition**|Yifei Wang et.al.|[2509.09940](http://arxiv.org/abs/2509.09940)|null|\n", "2509.09747": "|**2025-09-11**|**D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference**|Leen Daher et.al.|[2509.09747](http://arxiv.org/abs/2509.09747)|null|\n", "2509.06389": "|**2025-09-08**|**MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation**|Xiaoran Yang et.al.|[2509.06389](http://arxiv.org/abs/2509.06389)|null|\n", "2509.04957": "|**2025-09-05**|**Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper**|Gehui Chen et.al.|[2509.04957](http://arxiv.org/abs/2509.04957)|null|\n", "2509.04330": "|**2025-09-04**|**Temporal Interest-Driven Multimodal Personalized Content Generation**|Tian Miao et.al.|[2509.04330](http://arxiv.org/abs/2509.04330)|null|\n", "2508.20546": "|**2025-08-28**|**MM-HSD: Multi-Modal Hate Speech Detection in Videos**|Berta C\u00e9spedes-Sarrias et.al.|[2508.20546](http://arxiv.org/abs/2508.20546)|null|\n", "2508.16930": "|**2025-08-23**|**HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation**|Sizhe Shan et.al.|[2508.16930](http://arxiv.org/abs/2508.16930)|null|\n", "2508.16291": "|**2025-08-22**|**Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment**|Fengshun Wang et.al.|[2508.16291](http://arxiv.org/abs/2508.16291)|null|\n", "2508.13599": "|**2025-08-19**|**Towards Efficient Vision State Space Models via Token Merging**|Jinyoung Park et.al.|[2508.13599](http://arxiv.org/abs/2508.13599)|null|\n", "2508.13442": "|**2025-08-19**|**EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis**|Shuai Tan et.al.|[2508.13442](http://arxiv.org/abs/2508.13442)|null|\n", "2508.12918": "|**2025-08-21**|**FoleySpace: Vision-Aligned Binaural Spatial Audio Generation**|Lei Zhao et.al.|[2508.12918](http://arxiv.org/abs/2508.12918)|null|\n", "2508.12149": "|**2025-08-16**|**MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization**|Haochen You et.al.|[2508.12149](http://arxiv.org/abs/2508.12149)|null|\n", "2508.11362": "|**2025-08-15**|**Mitigating Category Imbalance: Fosafer System for the Multimodal Emotion and Intent Joint Understanding Challenge**|Honghong Wang et.al.|[2508.11362](http://arxiv.org/abs/2508.11362)|null|\n", "2508.11074": "|**2025-08-14**|**LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters**|Haomin Zhang et.al.|[2508.11074](http://arxiv.org/abs/2508.11074)|null|\n", "2508.10494": "|**2025-08-14**|**A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation**|Jiulin Li et.al.|[2508.10494](http://arxiv.org/abs/2508.10494)|null|\n", "2508.08199": "|**2025-08-11**|**Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model**|Peiqi He et.al.|[2508.08199](http://arxiv.org/abs/2508.08199)|null|\n", "2508.07558": "|**2025-08-11**|**UniFlow: Unifying Speech Front-End Tasks via Continuous Generative Modeling**|Ziqian Wang et.al.|[2508.07558](http://arxiv.org/abs/2508.07558)|null|\n", "2508.06057": "|**2025-08-08**|**AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?**|Mojtaba Valipour et.al.|[2508.06057](http://arxiv.org/abs/2508.06057)|null|\n", "2508.02429": "|**2025-08-04**|**Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting**|Miaosen Luo et.al.|[2508.02429](http://arxiv.org/abs/2508.02429)|null|\n", "2508.01181": "|**2025-10-11**|**Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning**|Zhiyuan Han et.al.|[2508.01181](http://arxiv.org/abs/2508.01181)|null|\n", "2508.00733": "|**2025-08-07**|**AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation**|Le Wang et.al.|[2508.00733](http://arxiv.org/abs/2508.00733)|null|\n", "2508.00632": "|**2025-08-01**|**Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings**|Alexia Jolicoeur-Martineau et.al.|[2508.00632](http://arxiv.org/abs/2508.00632)|null|\n", "2507.20368": "|**2025-07-27**|**MagicAnime: A Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation**|Shuolin Xu et.al.|[2507.20368](http://arxiv.org/abs/2507.20368)|null|\n", "2507.16206": "|**2025-07-22**|**METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark**|Xu Yang et.al.|[2507.16206](http://arxiv.org/abs/2507.16206)|null|\n", "2507.10109": "|**2025-07-14**|**DualDub: Video-to-Soundtrack Generation via Joint Speech and Background Audio Synthesis**|Wenjie Tian et.al.|[2507.10109](http://arxiv.org/abs/2507.10109)|null|\n", "2507.10069": "|**2025-11-11**|**ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism**|Zedong Liu et.al.|[2507.10069](http://arxiv.org/abs/2507.10069)|null|\n", "2507.09798": "|**2025-10-01**|**Dynamic Pacing for Real-time Satellite Traffic**|Aashish Gottipati et.al.|[2507.09798](http://arxiv.org/abs/2507.09798)|null|\n", "2507.21104": "|**2025-07-07**|**iLSU-T: an Open Dataset for Uruguayan Sign Language Translation**|Ariel E. Stassi et.al.|[2507.21104](http://arxiv.org/abs/2507.21104)|null|\n", "2507.04959": "|**2025-07-13**|**Hear-Your-Click: Interactive Object-Specific Video-to-Audio Generation**|Yingshan Liang et.al.|[2507.04959](http://arxiv.org/abs/2507.04959)|null|\n", "2507.04667": "|**2025-07-08**|**What's Making That Sound Right Now? Video-centric Audio-Visual Localization**|Hahyeon Choi et.al.|[2507.04667](http://arxiv.org/abs/2507.04667)|null|\n", "2507.04061": "|**2025-08-29**|**Consistent and Invariant Generalization Learning for Short-video Misinformation Detection**|Hanghui Guo et.al.|[2507.04061](http://arxiv.org/abs/2507.04061)|null|\n", "2507.02271": "|**2025-07-03**|**Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation**|Feizhen Huang et.al.|[2507.02271](http://arxiv.org/abs/2507.02271)|null|\n", "2506.22960": "|**2025-06-28**|**Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images**|Shreyas Dixit et.al.|[2506.22960](http://arxiv.org/abs/2506.22960)|null|\n", "2506.21448": "|**2025-11-05**|**ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing**|Huadai Liu et.al.|[2506.21448](http://arxiv.org/abs/2506.21448)|null|\n", "2506.20995": "|**2025-10-07**|**Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance**|Akio Hayakawa et.al.|[2506.20995](http://arxiv.org/abs/2506.20995)|null|\n", "2506.20816": "|**2025-06-25**|**Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers**|Furkan Mumcu et.al.|[2506.20816](http://arxiv.org/abs/2506.20816)|null|\n", "2506.19774": "|**2025-06-24**|**Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation**|Jun Wang et.al.|[2506.19774](http://arxiv.org/abs/2506.19774)|null|\n", "2506.17113": "|**2025-10-25**|**MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation**|Shoubin Yu et.al.|[2506.17113](http://arxiv.org/abs/2506.17113)|null|\n", "2506.12199": "|**2025-06-13**|**ViSAGe: Video-to-Spatial Audio Generation**|Jaeyeon Kim et.al.|[2506.12199](http://arxiv.org/abs/2506.12199)|null|\n", "2506.21556": "|**2025-09-26**|**VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation**|Hyeongcheol Park et.al.|[2506.21556](http://arxiv.org/abs/2506.21556)|null|\n", "2506.08634": "|**2025-06-10**|**MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback**|Alvaro Becerra et.al.|[2506.08634](http://arxiv.org/abs/2506.08634)|null|\n", "2506.08279": "|**2025-06-09**|**Seeing Voices: Generating A-Roll Video from Audio with Mirage**|Aditi Sundararaman et.al.|[2506.08279](http://arxiv.org/abs/2506.08279)|null|\n", "2506.08277": "|**2025-06-09**|**Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain**|Subba Reddy Oota et.al.|[2506.08277](http://arxiv.org/abs/2506.08277)|null|\n", "2506.05868": "|**2025-10-28**|**Detecting Coordinated Behaviour on Video-First Platforms: The Challenge of Multimodality and Complex Similarity on TikTok**|Inga K. Wohlert et.al.|[2506.05868](http://arxiv.org/abs/2506.05868)|null|\n", "2506.05851": "|**2025-06-06**|**DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection**|Marcel Klemt et.al.|[2506.05851](http://arxiv.org/abs/2506.05851)|null|\n", "2506.03682": "|**2025-12-15**|**How PARTs assemble into wholes: Learning the relative composition of images**|Melika Ayoughi et.al.|[2506.03682](http://arxiv.org/abs/2506.03682)|null|\n", "2506.01872": "|**2025-06-02**|**Is Extending Modality The Right Path Towards Omni-Modality?**|Tinghui Zhu et.al.|[2506.01872](http://arxiv.org/abs/2506.01872)|null|\n", "2505.23120": "|**2025-05-29**|**MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation**|Siyuan Wang et.al.|[2505.23120](http://arxiv.org/abs/2505.23120)|null|\n", "2505.22943": "|**2025-05-28**|**Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates**|Jaewoo Ahn et.al.|[2505.22943](http://arxiv.org/abs/2505.22943)|null|\n", "2505.21724": "|**2025-10-28**|**OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions**|Cheng Luo et.al.|[2505.21724](http://arxiv.org/abs/2505.21724)|null|\n", "2505.21002": "|**2025-05-27**|**Facial Attribute Based Text Guided Face Anonymization**|Mustafa \u0130zzet Mu\u015ftu et.al.|[2505.21002](http://arxiv.org/abs/2505.21002)|null|\n", "2505.20873": "|**2025-09-30**|**Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models**|Chaeyoung Jung et.al.|[2505.20873](http://arxiv.org/abs/2505.20873)|null|\n", "2505.20038": "|**2025-05-26**|**Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks**|Chang Liu et.al.|[2505.20038](http://arxiv.org/abs/2505.20038)|null|\n", "2505.20027": "|**2025-05-26**|**Multi-modal brain encoding models for multi-modal stimuli**|Subba Reddy Oota et.al.|[2505.20027](http://arxiv.org/abs/2505.20027)|null|\n", "2505.18876": "|**2025-05-24**|**DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets**|Maria Makarova et.al.|[2505.18876](http://arxiv.org/abs/2505.18876)|null|\n", "2505.16195": "|**2025-07-17**|**SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet**|Zhi Zhong et.al.|[2505.16195](http://arxiv.org/abs/2505.16195)|null|\n", "2505.14668": "|**2025-10-27**|**ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions**|Bufang Yang et.al.|[2505.14668](http://arxiv.org/abs/2505.14668)|null|\n", "2505.14151": "|**2025-06-04**|**ReactDiff: Latent Diffusion for Facial Reaction Generation**|Jiaming Li et.al.|[2505.14151](http://arxiv.org/abs/2505.14151)|null|\n", "2505.14741": "|**2025-10-13**|**Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism**|Kunyun Wang et.al.|[2505.14741](http://arxiv.org/abs/2505.14741)|null|\n", "2505.13062": "|**2025-05-28**|**Hearing from Silence: Reasoning Audio Descriptions from Silent Videos via Vision-Language Model**|Yong Ren et.al.|[2505.13062](http://arxiv.org/abs/2505.13062)|null|\n", "2505.12051": "|**2025-05-17**|**Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion**|Yinghui Zhang et.al.|[2505.12051](http://arxiv.org/abs/2505.12051)|null|\n", "2505.08202": "|**2025-05-13**|**AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques**|Aman Raj et.al.|[2505.08202](http://arxiv.org/abs/2505.08202)|null|\n", "2505.05681": "|**2025-05-08**|**Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos**|Giulio Cesare Mastrocinque Santo et.al.|[2505.05681](http://arxiv.org/abs/2505.05681)|null|\n", "2505.03084": "|**2025-05-06**|**Adversarial Attacks in Multimodal Systems: A Practitioner's Survey**|Shashank Kapoor et.al.|[2505.03084](http://arxiv.org/abs/2505.03084)|null|\n", "2505.02466": "|**2025-05-05**|**Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language, and Modality**|Xueguang Ma et.al.|[2505.02466](http://arxiv.org/abs/2505.02466)|null|\n", "2504.19549": "|**2025-10-25**|**DEEMO: De-identity Multimodal Emotion Recognition and Reasoning**|Deng Li et.al.|[2504.19549](http://arxiv.org/abs/2504.19549)|null|\n", "2504.14906": "|**2025-06-03**|**OmniAudio: Generating Spatial Audio from 360-Degree Video**|Huadai Liu et.al.|[2504.14906](http://arxiv.org/abs/2504.14906)|null|\n", "2504.14214": "|**2025-04-19**|**Teach Me How to Denoise: A Universal Framework for Denoising Multi-modal Recommender Systems via Guided Calibration**|Hongji Li et.al.|[2504.14214](http://arxiv.org/abs/2504.14214)|null|\n", "2504.12315": "|**2025-04-10**|**Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models**|Xingguang Ji et.al.|[2504.12315](http://arxiv.org/abs/2504.12315)|null|\n", "2504.06778": "|**2025-04-17**|**CAFA: a Controllable Automatic Foley Artist**|Roi Benita et.al.|[2504.06778](http://arxiv.org/abs/2504.06778)|null|\n", "2504.05684": "|**2025-10-10**|**TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis**|Tri Ton et.al.|[2504.05684](http://arxiv.org/abs/2504.05684)|null|\n", "2504.02988": "|**2025-04-03**|**Generating Diverse Audio-Visual 360 Soundscapes for Sound Event Localization and Detection**|Adrian S. Roman et.al.|[2504.02988](http://arxiv.org/abs/2504.02988)|null|\n", "2504.01470": "|**2025-04-02**|**Detecting Lip-Syncing Deepfakes: Vision Temporal Transformer for Analyzing Mouth Inconsistencies**|Soumyya Kanti Datta et.al.|[2504.01470](http://arxiv.org/abs/2504.01470)|null|\n", "2504.00221": "|**2025-03-31**|**GazeLLM: Multimodal LLMs incorporating Human Visual Attention**|Jun Rekimoto et.al.|[2504.00221](http://arxiv.org/abs/2504.00221)|null|\n", "2504.01032": "|**2025-03-29**|**Who Owns the Output? Bridging Law and Technology in LLMs Attribution**|Emanuele Mezzi et.al.|[2504.01032](http://arxiv.org/abs/2504.01032)|null|\n", "2504.01027": "|**2025-03-28**|**Mesh Compression with Quantized Neural Displacement Fields**|Sai Karthikey Pentapati et.al.|[2504.01027](http://arxiv.org/abs/2504.01027)|null|\n", "2503.22265": "|**2025-03-28**|**DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech and Audio Generation**|Haomin Zhang et.al.|[2503.22265](http://arxiv.org/abs/2503.22265)|null|\n", "2503.22200": "|**2025-03-28**|**Enhance Generation Quality of Flow Matching V2A Model via Multi-Step CoT-Like Guidance and Combined Preference Optimization**|Haomin Zhang et.al.|[2503.22200](http://arxiv.org/abs/2503.22200)|null|\n", "2503.21775": "|**2025-03-27**|**StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion**|Ziyu Guo et.al.|[2503.21775](http://arxiv.org/abs/2503.21775)|null|\n", "2503.21699": "|**2025-12-06**|**MAVERIX: Multimodal Audio-Visual Evaluation and Recognition IndeX**|Liuyue Xie et.al.|[2503.21699](http://arxiv.org/abs/2503.21699)|null|\n", "2503.21848": "|**2025-03-27**|**Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation**|Jonathan Attard et.al.|[2503.21848](http://arxiv.org/abs/2503.21848)|null|\n", "2503.20853": "|**2025-03-26**|**Unified Multimodal Discrete Diffusion**|Alexander Swerdlow et.al.|[2503.20853](http://arxiv.org/abs/2503.20853)|null|\n", "2503.19416": "|**2025-04-02**|**EmoHead: Emotional Talking Head via Manipulating Semantic Expression Parameters**|Xuli Shen et.al.|[2503.19416](http://arxiv.org/abs/2503.19416)|null|\n", "2503.18223": "|**2025-06-04**|**MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps**|Valentin Gabeff et.al.|[2503.18223](http://arxiv.org/abs/2503.18223)|null|\n", "2503.17116": "|**2025-03-21**|**The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding**|Luca Rossetto et.al.|[2503.17116](http://arxiv.org/abs/2503.17116)|null|\n", "2503.16311": "|**2025-03-20**|**Structured-Noise Masked Modeling for Video, Audio and Beyond**|Aritra Bhowmik et.al.|[2503.16311](http://arxiv.org/abs/2503.16311)|null|\n", "2503.14505": "|**2025-12-13**|**MusicInfuser: Making Video Diffusion Listen and Dance**|Susung Hong et.al.|[2503.14505](http://arxiv.org/abs/2503.14505)|null|\n", "2503.14504": "|**2025-03-23**|**Aligning Multimodal LLM with Human Preference: A Survey**|Tao Yu et.al.|[2503.14504](http://arxiv.org/abs/2503.14504)|null|\n", "2503.14295": "|**2025-03-20**|**PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation**|Baiqin Wang et.al.|[2503.14295](http://arxiv.org/abs/2503.14295)|null|\n", "2503.12623": "|**2025-05-02**|**MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network**|Vrushank Ahire et.al.|[2503.12623](http://arxiv.org/abs/2503.12623)|null|\n", "2503.12605": "|**2025-03-23**|**Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey**|Yaoting Wang et.al.|[2503.12605](http://arxiv.org/abs/2503.12605)|null|\n", "2503.12131": "|**2025-03-15**|**DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap**|Shentong Mo et.al.|[2503.12131](http://arxiv.org/abs/2503.12131)|null|\n", "2503.10719": "|**2025-03-17**|**Long-Video Audio Synthesis with Multi-Agent Collaboration**|Yehang Zhang et.al.|[2503.10719](http://arxiv.org/abs/2503.10719)|null|\n", "2503.09205": "|**2025-11-10**|**Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model**|Ali Vosoughi et.al.|[2503.09205](http://arxiv.org/abs/2503.09205)|null|\n", "2503.10700": "|**2025-03-12**|**TA-V2A: Textually Assisted Video-to-Audio Generation**|Yuhuan You et.al.|[2503.10700](http://arxiv.org/abs/2503.10700)|null|\n", "2503.06984": "|**2025-03-10**|**Synchronized Video-to-Audio Generation via Mel Quantization-Continuum Decomposition**|Juncheng Wang et.al.|[2503.06984](http://arxiv.org/abs/2503.06984)|null|\n", "2503.06134": "|**2025-08-09**|**X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation**|Jian Ma et.al.|[2503.06134](http://arxiv.org/abs/2503.06134)|null|\n", "2503.03987": "|**2025-03-06**|**RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered by Large Vision-Language Models**|Wenhui Zhu et.al.|[2503.03987](http://arxiv.org/abs/2503.03987)|null|\n", "2503.02284": "|**2025-03-04**|**Semi-Supervised Audio-Visual Video Action Recognition with Audio Source Localization Guided Mixup**|Seokun Kang et.al.|[2503.02284](http://arxiv.org/abs/2503.02284)|null|\n", "2503.00364": "|**2025-03-01**|**CFSum: A Transformer-Based Multi-Modal Video Summarization Framework With Coarse-Fine Fusion**|Yaowei Guo et.al.|[2503.00364](http://arxiv.org/abs/2503.00364)|null|\n", "2502.19455": "|**2025-04-23**|**FLAP: Fully-controllable Audio-driven Portrait Video Generation through 3D head conditioned diffusion model**|Lingzhou Mu et.al.|[2502.19455](http://arxiv.org/abs/2502.19455)|null|\n", "2502.14178": "|**2025-02-20**|**NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis**|Xiaoxing Liu et.al.|[2502.14178](http://arxiv.org/abs/2502.14178)|null|\n", "2502.12562": "|**2025-06-02**|**SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings**|Weikai Lu et.al.|[2502.12562](http://arxiv.org/abs/2502.12562)|null|\n", "2502.12425": "|**2025-02-18**|**Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning**|Mengshi Qi et.al.|[2502.12425](http://arxiv.org/abs/2502.12425)|null|\n", "2502.06782": "|**2025-02-12**|**Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT**|Dongyang Liu et.al.|[2502.06782](http://arxiv.org/abs/2502.06782)|null|\n", "2502.06020": "|**2025-02-09**|**Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding**|Xingjian Diao et.al.|[2502.06020](http://arxiv.org/abs/2502.06020)|null|\n", "2502.05673": "|**2025-07-03**|**The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions**|Ping Liu et.al.|[2502.05673](http://arxiv.org/abs/2502.05673)|null|\n", "2502.04328": "|**2025-06-02**|**Ola: Pushing the Frontiers of Omni-Modal Language Model**|Zuyan Liu et.al.|[2502.04328](http://arxiv.org/abs/2502.04328)|null|\n", "2502.03897": "|**2025-07-07**|**UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation**|Lei Zhao et.al.|[2502.03897](http://arxiv.org/abs/2502.03897)|null|\n", "2502.03862": "|**2025-02-07**|**Enhancing Deliberativeness: Evaluating the Impact of Multimodal Reflection Nudges**|ShunYi Yeo et.al.|[2502.03862](http://arxiv.org/abs/2502.03862)|null|\n", "2502.03005": "|**2025-02-05**|**Driver Assistance System Based on Multimodal Data Hazard Detection**|Long Zhouxiang et.al.|[2502.03005](http://arxiv.org/abs/2502.03005)|null|\n", "2502.01547": "|**2025-05-07**|**mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition**|Andrew Rouditchenko et.al.|[2502.01547](http://arxiv.org/abs/2502.01547)|null|\n", "2502.06805": "|**2025-06-06**|**Efficient Diffusion Models: A Survey**|Hui Shen et.al.|[2502.06805](http://arxiv.org/abs/2502.06805)|null|\n", "2502.00937": "|**2025-10-22**|**ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving**|Haoran Qiu et.al.|[2502.00937](http://arxiv.org/abs/2502.00937)|null|\n", "2502.00702": "|**2025-12-10**|**CardioLive: Empowering Video Streaming with Online Cardiac Monitoring**|Sheng Lyu et.al.|[2502.00702](http://arxiv.org/abs/2502.00702)|null|\n", "2501.18314": "|**2025-07-14**|**AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment**|Yuqin Cao et.al.|[2501.18314](http://arxiv.org/abs/2501.18314)|null|\n", "2501.18096": "|**2025-01-30**|**LLMs can see and hear without any training**|Kumar Ashutosh et.al.|[2501.18096](http://arxiv.org/abs/2501.18096)|null|\n", "2502.15711": "|**2025-01-22**|**A Survey on Multimodal Recommender Systems: Recent Advances and Future Directions**|Jinfeng Xu et.al.|[2502.15711](http://arxiv.org/abs/2502.15711)|null|\n", "2501.05098": "|**2025-01-09**|**Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset**|Yuhong Zhang et.al.|[2501.05098](http://arxiv.org/abs/2501.05098)|null|\n", "2412.20378": "|**2024-12-29**|**Tri-Ergon: Fine-grained Video-to-Audio Generation with Multi-modal Conditions and LUFS Control**|Bingliang Li et.al.|[2412.20378](http://arxiv.org/abs/2412.20378)|null|\n", "2412.19999": "|**2024-12-28**|**Comprehensive Review of EEG-to-Output Research: Decoding Neural Signals into Images, Videos, and Audio**|Yashvir Sabharwal et.al.|[2412.19999](http://arxiv.org/abs/2412.19999)|null|\n", "2412.18390": "|**2024-12-25**|**RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction**|Xiaoping Wu et.al.|[2412.18390](http://arxiv.org/abs/2412.18390)|null|\n", "2412.18157": "|**2024-12-24**|**Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance**|Yaoyun Zhang et.al.|[2412.18157](http://arxiv.org/abs/2412.18157)|null|\n", "2501.14755": "|**2025-10-29**|**Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models**|Daoyuan Chen et.al.|[2501.14755](http://arxiv.org/abs/2501.14755)|null|\n", "2412.15322": "|**2025-04-07**|**MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis**|Ho Kei Cheng et.al.|[2412.15322](http://arxiv.org/abs/2412.15322)|null|\n", "2412.15191": "|**2025-03-10**|**AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation**|Moayed Haji-Ali et.al.|[2412.15191](http://arxiv.org/abs/2412.15191)|null|\n", "2412.13462": "|**2024-12-18**|**SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation**|Kazuki Shimada et.al.|[2412.13462](http://arxiv.org/abs/2412.13462)|null|\n", "2412.12781": "|**2024-12-17**|**Predicting change in time production -- A machine learning approach to time perception**|Amrapali Pednekar et.al.|[2412.12781](http://arxiv.org/abs/2412.12781)|null|\n", "2412.10768": "|**2024-12-14**|**VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation**|Saksham Singh Kushwaha et.al.|[2412.10768](http://arxiv.org/abs/2412.10768)|null|\n", "2412.09596": "|**2024-12-12**|**InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions**|Pan Zhang et.al.|[2412.09596](http://arxiv.org/abs/2412.09596)|null|\n", "2412.09317": "|**2024-12-12**|**Multimodal Sentiment Analysis based on Video and Audio Inputs**|Antonio Fernandez et.al.|[2412.09317](http://arxiv.org/abs/2412.09317)|null|\n", "2412.09168": "|**2024-12-12**|**YingSound: Video-Guided Sound Effects Generation with Multi-modal Chain-of-Thought Controls**|Zihao Chen et.al.|[2412.09168](http://arxiv.org/abs/2412.09168)|null|\n", "2412.08529": "|**2024-12-11**|**TECO: Improving Multimodal Intent Recognition with Text Enhancement through Commonsense Knowledge Extraction**|Quynh-Mai Thi Nguyen et.al.|[2412.08529](http://arxiv.org/abs/2412.08529)|null|\n", "2412.06208": "|**2024-12-09**|**Pilot-guided Multimodal Semantic Communication for Audio-Visual Event Localization**|Fei Yu et.al.|[2412.06208](http://arxiv.org/abs/2412.06208)|null|\n", "2412.15220": "|**2024-12-03**|**SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from Text**|Haohe Liu et.al.|[2412.15220](http://arxiv.org/abs/2412.15220)|null|\n", "2411.19245": "|**2024-11-28**|**Contrastive representations of high-dimensional, structured treatments**|Oriol Corcoll Andreu et.al.|[2411.19245](http://arxiv.org/abs/2411.19245)|null|\n", "2411.16613": "|**2024-11-25**|**Recent Trends in Linear Text Segmentation: a Survey**|Iacopo Ghinassi et.al.|[2411.16613](http://arxiv.org/abs/2411.16613)|null|\n", "2411.15457": "|**2024-11-23**|**Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset**|Sukhandeep Kaur et.al.|[2411.15457](http://arxiv.org/abs/2411.15457)|null|\n", "2411.15447": "|**2025-08-12**|**Gotta Hear Them All: Towards Sound Source Aware Audio Generation**|Wei Guo et.al.|[2411.15447](http://arxiv.org/abs/2411.15447)|null|\n", "2411.06284": "|**2025-11-28**|**A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks**|Chia Xin Liang et.al.|[2411.06284](http://arxiv.org/abs/2411.06284)|null|\n", "2411.05903": "|**2024-11-08**|**Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model**|Ben Koska et.al.|[2411.05903](http://arxiv.org/abs/2411.05903)|null|\n", "2411.05679": "|**2025-04-04**|**Tell What You Hear From What You See -- Video to Audio Generation Through Text**|Xiulong Liu et.al.|[2411.05679](http://arxiv.org/abs/2411.05679)|null|\n", "2411.05023": "|**2024-10-29**|**Multimodal Quantum Natural Language Processing: A Novel Framework for using Quantum Methods to Analyse Real Data**|Hala Hawashin et.al.|[2411.05023](http://arxiv.org/abs/2411.05023)|null|\n", "2410.22271": "|**2024-10-29**|**Leveraging Reverberation and Visual Depth Cues for Sound Event Localization and Detection with Distance Estimation**|Davide Berghi et.al.|[2410.22271](http://arxiv.org/abs/2410.22271)|null|\n", "2410.22112": "|**2024-10-29**|**Multimodal Semantic Communication for Generative Audio-Driven Video Conferencing**|Haonan Tong et.al.|[2410.22112](http://arxiv.org/abs/2410.22112)|null|\n", "2410.17442": "|**2025-06-16**|**Detecting Adversarial Examples**|Furkan Mumcu et.al.|[2410.17442](http://arxiv.org/abs/2410.17442)|null|\n", "2410.16733": "|**2024-10-22**|**50 questions on Active Assisted Living technologies. Global edition**|Francisco Florez-Revuelta et.al.|[2410.16733](http://arxiv.org/abs/2410.16733)|null|\n", "2410.16592": "|**2024-10-22**|**ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding**|Andrew Kan et.al.|[2410.16592](http://arxiv.org/abs/2410.16592)|null|\n", "2410.16438": "|**2024-10-21**|**AlignVSR: Audio-Visual Cross-Modal Alignment for Visual Speech Recognition**|Zehua Liu et.al.|[2410.16438](http://arxiv.org/abs/2410.16438)|null|\n", "2410.15029": "|**2025-03-24**|**Enhancing Multimodal Sentiment Analysis for Missing Modality through Self-Distillation and Unified Modality Cross-Attention**|Yuzhe Weng et.al.|[2410.15029](http://arxiv.org/abs/2410.15029)|null|\n", "2410.13720": "|**2025-02-26**|**Movie Gen: A Cast of Media Foundation Models**|Adam Polyak et.al.|[2410.13720](http://arxiv.org/abs/2410.13720)|null|\n", "2410.13638": "|**2024-10-17**|**Scaling Wearable Foundation Models**|Girish Narayanswamy et.al.|[2410.13638](http://arxiv.org/abs/2410.13638)|null|\n", "2410.12787": "|**2024-10-16**|**The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio**|Sicong Leng et.al.|[2410.12787](http://arxiv.org/abs/2410.12787)|null|\n", "2410.19760": "|**2024-10-11**|**Movie Trailer Genre Classification Using Multimodal Pretrained Features**|Serkan Sulun et.al.|[2410.19760](http://arxiv.org/abs/2410.19760)|null|\n", "2410.09088": "|**2024-10-08**|**The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024**|Yinan Han et.al.|[2410.09088](http://arxiv.org/abs/2410.09088)|null|\n", "2410.03487": "|**2024-10-04**|**A Multimodal Framework for Deepfake Detection**|Kashish Gandhi et.al.|[2410.03487](http://arxiv.org/abs/2410.03487)|null|\n", "2410.03412": "|**2024-10-04**|**Team MTS @ AutoMin 2021: An Overview of Existing Summarization Approaches and Comparison to Unsupervised Summarization Techniques**|Olga Iakovenko et.al.|[2410.03412](http://arxiv.org/abs/2410.03412)|null|\n", "2410.03335": "|**2025-01-14**|**Audio-Agent: Leveraging LLMs For Audio Generation, Editing and Composition**|Zixuan Wang et.al.|[2410.03335](http://arxiv.org/abs/2410.03335)|null|\n", "2410.01594": "|**2024-10-02**|**MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation**|Mingzhen Sun et.al.|[2410.01594](http://arxiv.org/abs/2410.01594)|null|\n", "2410.03755": "|**2025-02-04**|**Denoising with a Joint-Embedding Predictive Architecture**|Dengsheng Chen et.al.|[2410.03755](http://arxiv.org/abs/2410.03755)|null|\n", "2409.19132": "|**2024-09-27**|**From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation**|Kun Su et.al.|[2409.19132](http://arxiv.org/abs/2409.19132)|null|\n", "2409.15157": "|**2024-12-30**|**LoVA: Long-form Video-to-Audio Generation**|Xin Cheng et.al.|[2409.15157](http://arxiv.org/abs/2409.15157)|null|\n", "2409.14709": "|**2024-09-23**|**Video-to-Audio Generation with Fine-grained Temporal Semantics**|Yuchen Hu et.al.|[2409.14709](http://arxiv.org/abs/2409.14709)|null|\n", "2409.13689": "|**2024-09-20**|**Temporally Aligned Audio for Video with Autoregression**|Ilpo Viertola et.al.|[2409.13689](http://arxiv.org/abs/2409.13689)|null|\n", "2409.09823": "|**2024-09-15**|**Efficient Video to Audio Mapper with Visual Scene Detection**|Mingjing Yi et.al.|[2409.09823](http://arxiv.org/abs/2409.09823)|null|\n", "2409.08628": "|**2024-09-13**|**Rhythmic Foley: A Framework For Seamless Audio-Visual Alignment In Video-to-Audio Synthesis**|Zhiqi Huang et.al.|[2409.08628](http://arxiv.org/abs/2409.08628)|null|\n", "2409.08601": "|**2025-03-24**|**STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment**|Yong Ren et.al.|[2409.08601](http://arxiv.org/abs/2409.08601)|null|\n", "2409.06135": "|**2024-09-10**|**Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis**|Qi Yang et.al.|[2409.06135](http://arxiv.org/abs/2409.06135)|null|\n", "2409.03597": "|**2025-04-22**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Fold Paralysis**|Yucong Zhang et.al.|[2409.03597](http://arxiv.org/abs/2409.03597)|null|\n", "2409.00851": "|**2024-09-01**|**Dissecting Temporal Understanding in Text-to-Audio Retrieval**|Andreea-Maria Oncescu et.al.|[2409.00851](http://arxiv.org/abs/2409.00851)|null|\n", "2408.16703": "|**2025-02-17**|**RoboMNIST: A Multimodal Dataset for Multi-Robot Activity Recognition Using WiFi Sensing, Video, and Audio**|Kian Behzad et.al.|[2408.16703](http://arxiv.org/abs/2408.16703)|null|\n", "2408.15769": "|**2024-08-28**|**A Survey on Evaluation of Multimodal Large Language Models**|Jiaxing Huang et.al.|[2408.15769](http://arxiv.org/abs/2408.15769)|null|\n", "2408.14595": "|**2025-11-18**|**Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models**|Ian Stewart et.al.|[2408.14595](http://arxiv.org/abs/2408.14595)|null|\n", "2408.03185": "|**2024-08-06**|**MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and Maximizing Utility in Audio-Visual Data Archiving**|Babajide Alamu Owoyele et.al.|[2408.03185](http://arxiv.org/abs/2408.03185)|null|\n", "2408.02954": "|**2024-08-06**|**WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal Deepfake Detection**|Juho Jung et.al.|[2408.02954](http://arxiv.org/abs/2408.02954)|null|\n", "2407.21757": "|**2024-09-12**|**Learning Video Context as Interleaved Multimodal Sequences**|Kevin Qinghong Lin et.al.|[2407.21757](http://arxiv.org/abs/2407.21757)|null|\n", "2407.20592": "|**2024-12-14**|**EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos**|Aashish Rai et.al.|[2407.20592](http://arxiv.org/abs/2407.20592)|null|\n", "2407.19704": "|**2024-07-29**|**UNQA: Unified No-Reference Quality Assessment for Audio, Image, Video, and Audio-Visual Content**|Yuqin Cao et.al.|[2407.19704](http://arxiv.org/abs/2407.19704)|null|\n", "2407.11820": "|**2024-09-12**|**Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation**|Juncheng Ma et.al.|[2407.11820](http://arxiv.org/abs/2407.11820)|null|\n", "2407.10387": "|**2024-07-15**|**Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity**|Santiago Pascual et.al.|[2407.10387](http://arxiv.org/abs/2407.10387)|null|\n", "2407.09147": "|**2024-07-12**|**AI-Powered Immersive Assistance for Interactive Task Execution in Industrial Environments**|Tomislav Duricic et.al.|[2407.09147](http://arxiv.org/abs/2407.09147)|null|\n", "2407.08949": "|**2024-07-12**|**One-Shot Pose-Driving Face Animation Platform**|He Feng et.al.|[2407.08949](http://arxiv.org/abs/2407.08949)|null|\n", "2407.07464": "|**2025-03-11**|**Video-to-Audio Generation with Hidden Alignment**|Manjie Xu et.al.|[2407.07464](http://arxiv.org/abs/2407.07464)|null|\n", "2407.06174": "|**2024-08-21**|**The Tug-of-War Between Deepfake Generation and Detection**|Hannah Lee et.al.|[2407.06174](http://arxiv.org/abs/2407.06174)|null|\n", "2407.05712": "|**2025-04-08**|**MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices**|Jianwen Jiang et.al.|[2407.05712](http://arxiv.org/abs/2407.05712)|null|\n", "2407.05551": "|**2024-12-26**|**Read, Watch and Scream! Sound Generation from Text and Video**|Yujin Jeong et.al.|[2407.05551](http://arxiv.org/abs/2407.05551)|null|\n", "2407.01494": "|**2024-07-01**|**FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds**|Yiming Zhang et.al.|[2407.01494](http://arxiv.org/abs/2407.01494)|null|\n", "2407.00603": "|**2024-12-15**|**Hierarchical Memory for Long Video QA**|Yiqin Wang et.al.|[2407.00603](http://arxiv.org/abs/2407.00603)|null|\n", "2406.20098": "|**2024-11-17**|**Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs**|Sukmin Yun et.al.|[2406.20098](http://arxiv.org/abs/2406.20098)|null|\n", "2407.00119": "|**2024-08-31**|**Efficient Long-distance Latent Relation-aware Graph Neural Network for Multi-modal Emotion Recognition in Conversations**|Yuntao Shou et.al.|[2407.00119](http://arxiv.org/abs/2407.00119)|null|\n", "2406.18131": "|**2024-06-26**|**Sequential Disentanglement by Extracting Static Information From A Single Sequence Element**|Nimrod Berman et.al.|[2406.18131](http://arxiv.org/abs/2406.18131)|null|\n", "2406.14176": "|**2024-08-19**|**A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection**|Kyungbok Lee et.al.|[2406.14176](http://arxiv.org/abs/2406.14176)|null|\n", "2406.09272": "|**2024-07-25**|**Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos**|Changan Chen et.al.|[2406.09272](http://arxiv.org/abs/2406.09272)|null|\n", "2406.07686": "|**2024-06-11**|**AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation**|Kai Wang et.al.|[2406.07686](http://arxiv.org/abs/2406.07686)|null|\n", "2406.07476": "|**2024-10-30**|**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**|Zesen Cheng et.al.|[2406.07476](http://arxiv.org/abs/2406.07476)|null|\n", "2406.06964": "|**2024-06-11**|**Missingness-resilient Video-enhanced Multimodal Disfluency Detection**|Payal Mohapatra et.al.|[2406.06964](http://arxiv.org/abs/2406.06964)|null|\n", "2406.05109": "|**2024-06-07**|**Large Generative Graph Models**|Yu Wang et.al.|[2406.05109](http://arxiv.org/abs/2406.05109)|null|\n", "2406.00320": "|**2025-01-04**|**Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching**|Yongqi Wang et.al.|[2406.00320](http://arxiv.org/abs/2406.00320)|null|\n", "2405.19335": "|**2024-05-29**|**X-VILA: Cross-Modality Alignment for Large Language Model**|Hanrong Ye et.al.|[2405.19335](http://arxiv.org/abs/2405.19335)|null|\n", "2405.16701": "|**2024-05-26**|**Detail-Enhanced Intra- and Inter-modal Interaction for Audio-Visual Emotion Recognition**|Tong Shi et.al.|[2405.16701](http://arxiv.org/abs/2405.16701)|null|\n", "2405.16136": "|**2024-05-25**|**C3LLM: Conditional Multimodal Content Generation Using Large Language Models**|Zixuan Wang et.al.|[2405.16136](http://arxiv.org/abs/2405.16136)|null|\n", "2405.13708": "|**2024-05-22**|**Requirements are All You Need: The Final Frontier for End-User Software Engineering**|Diana Robinson et.al.|[2405.13708](http://arxiv.org/abs/2405.13708)|null|\n", "2405.09266": "|**2024-11-28**|**Dance Any Beat: Blending Beats with Visuals in Dance Video Generation**|Xuanchen Wang et.al.|[2405.09266](http://arxiv.org/abs/2405.09266)|null|\n", "2405.09589": "|**2024-10-03**|**A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models**|Pranab Sahoo et.al.|[2405.09589](http://arxiv.org/abs/2405.09589)|null|\n", "2405.07202": "|**2024-05-12**|**Unified Video-Language Pre-training with Synchronized Audio**|Shentong Mo et.al.|[2405.07202](http://arxiv.org/abs/2405.07202)|null|\n", "2405.06128": "|**2024-05-09**|**Enhanced Multimodal Content Moderation of Children's Videos using Audiovisual Fusion**|Syed Hammad Ahmed et.al.|[2405.06128](http://arxiv.org/abs/2405.06128)|null|\n", "2405.03150": "|**2024-11-17**|**Video Diffusion Models: A Survey**|Andrew Melnik et.al.|[2405.03150](http://arxiv.org/abs/2405.03150)|null|\n", "2404.19040": "|**2024-04-29**|**GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable Gaussian Splatting**|Bo Chen et.al.|[2404.19040](http://arxiv.org/abs/2404.19040)|null|\n", "2404.16305": "|**2024-04-26**|**Semantically consistent Video-to-Audio Generation using Multimodal Language Large Model**|Gehui Chen et.al.|[2404.16305](http://arxiv.org/abs/2404.16305)|null|\n", "2404.13146": "|**2024-06-27**|**DeepFake-O-Meter v2.0: An Open Platform for DeepFake Detection**|Yan Ju et.al.|[2404.13146](http://arxiv.org/abs/2404.13146)|null|\n", "2404.12387": "|**2024-04-18**|**Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models**|Reka Team et.al.|[2404.12387](http://arxiv.org/abs/2404.12387)|null|\n", "2404.06309": "|**2024-04-09**|**Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models**|David Kurzend\u00f6rfer et.al.|[2404.06309](http://arxiv.org/abs/2404.06309)|null|\n", "2404.01709": "|**2024-04-02**|**Upsample Guidance: Scale Up Diffusion Models without Training**|Juno Hwang et.al.|[2404.01709](http://arxiv.org/abs/2404.01709)|null|\n", "2404.01647": "|**2024-04-02**|**EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis**|Shuai Tan et.al.|[2404.01647](http://arxiv.org/abs/2404.01647)|null|\n", "2404.01322": "|**2024-03-28**|**A Review of Multi-Modal Large Language and Vision Models**|Kilian Carolan et.al.|[2404.01322](http://arxiv.org/abs/2404.01322)|null|\n", "2403.17883": "|**2024-03-26**|**Superior and Pragmatic Talking Face Generation with Teacher-Student Framework**|Chao Liang et.al.|[2403.17883](http://arxiv.org/abs/2403.17883)|null|\n", "2403.16276": "|**2025-10-08**|**Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding**|Yolo Yunlong Tang et.al.|[2403.16276](http://arxiv.org/abs/2403.16276)|null|\n", "2403.16055": "|**2024-04-21**|**Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM**|Kun Ouyang et.al.|[2403.16055](http://arxiv.org/abs/2403.16055)|null|\n", "2403.15377": "|**2024-08-14**|**InternVideo2: Scaling Foundation Models for Multimodal Video Understanding**|Yi Wang et.al.|[2403.15377](http://arxiv.org/abs/2403.15377)|null|\n", "2403.13499": "|**2024-03-20**|**Improved Baselines for Data-efficient Perceptual Augmentation of LLMs**|Th\u00e9ophane Vallaeys et.al.|[2403.13499](http://arxiv.org/abs/2403.13499)|null|\n", "2403.15469": "|**2024-03-20**|**Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning**|Shivam Ratnakant Mhaskar et.al.|[2403.15469](http://arxiv.org/abs/2403.15469)|null|\n", "2403.12609": "|**2024-03-19**|**SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition**|Denis Dresvyanskiy et.al.|[2403.12609](http://arxiv.org/abs/2403.12609)|null|\n", "2403.12747": "|**2024-03-18**|**N-Modal Contrastive Losses with Applications to Social Media Data in Trimodal Space**|William Theisen et.al.|[2403.12747](http://arxiv.org/abs/2403.12747)|null|\n", "2403.10565": "|**2024-03-14**|**PTSD-MDNN : Fusion tardive de r\u00e9seaux de neurones profonds multimodaux pour la d\u00e9tection du trouble de stress post-traumatique**|Long Nguyen-Phuoc et.al.|[2403.10565](http://arxiv.org/abs/2403.10565)|null|\n", "2403.06576": "|**2024-03-11**|**FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder**|Yang Chen et.al.|[2403.06576](http://arxiv.org/abs/2403.06576)|null|\n", "2403.05530": "|**2024-12-16**|**Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context**|Gemini Team et.al.|[2403.05530](http://arxiv.org/abs/2403.05530)|null|\n", "2403.01133": "|**2024-04-14**|**Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data**|Aritra Hota et.al.|[2403.01133](http://arxiv.org/abs/2403.01133)|null|\n", "2402.19329": "|**2024-04-18**|**Social Links vs. Language Barriers: Decoding the Global Spread of Streaming Content**|Seoyoung Park et.al.|[2402.19329](http://arxiv.org/abs/2402.19329)|null|\n", "2402.18702": "|**2024-02-28**|**Characterizing Multimedia Information Environment through Multi-modal Clustering of YouTube Videos**|Niloofar Yousefi et.al.|[2402.18702](http://arxiv.org/abs/2402.18702)|null|\n", "2402.17723": "|**2024-02-27**|**Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners**|Yazhou Xing et.al.|[2402.17723](http://arxiv.org/abs/2402.17723)|null|\n", "2402.14891": "|**2026-01-28**|**LLMBind: A Unified Modality-Task Integration Framework**|Bin Zhu et.al.|[2402.14891](http://arxiv.org/abs/2402.14891)|null|\n", "2402.14843": "|**2024-02-19**|**Text Diffusion with Reinforced Conditioning**|Yuxuan Liu et.al.|[2402.14843](http://arxiv.org/abs/2402.14843)|null|\n", "2402.04739": "|**2024-02-07**|**Application-Layer FEC Scheme Configuration Optimization via Hybrid Simulated Annealing**|C\u00e9sar D\u00edaz et.al.|[2402.04739](http://arxiv.org/abs/2402.04739)|null|\n", "2402.03658": "|**2025-01-06**|**Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue**|Kun Ouyang et.al.|[2402.03658](http://arxiv.org/abs/2402.03658)|null|\n", "2402.01965": "|**2024-05-22**|**Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization**|Fangzhao Zhang et.al.|[2402.01965](http://arxiv.org/abs/2402.01965)|null|\n", "2402.01791": "|**2024-02-02**|**Variational Quantum Circuits Enhanced Generative Adversarial Network**|Runqiu Shu et.al.|[2402.01791](http://arxiv.org/abs/2402.01791)|null|\n", "2401.14405": "|**2024-03-18**|**Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities**|Yiyuan Zhang et.al.|[2401.14405](http://arxiv.org/abs/2401.14405)|null|\n", "2401.12972": "|**2024-08-29**|**On the Efficacy of Text-Based Input Modalities for Action Anticipation**|Apoorva Beedu et.al.|[2401.12972](http://arxiv.org/abs/2401.12972)|null|\n", "2401.09774": "|**2024-01-18**|**On the Audio Hallucinations in Large Audio-Video Language Models**|Taichi Nishimura et.al.|[2401.09774](http://arxiv.org/abs/2401.09774)|null|\n", "2401.09759": "|**2024-07-02**|**SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition**|Hao Wang et.al.|[2401.09759](http://arxiv.org/abs/2401.09759)|null|\n", "2401.08503": "|**2024-03-23**|**Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**|Zhenhui Ye et.al.|[2401.08503](http://arxiv.org/abs/2401.08503)|null|\n", "2401.08721": "|**2024-01-16**|**A Telerehabilitation System for the Selection, Evaluation and Remote Management of Therapies**|David Anton et.al.|[2401.08721](http://arxiv.org/abs/2401.08721)|null|\n", "2401.06395": "|**2024-04-04**|**ModaVerse: Efficiently Transforming Modalities with LLMs**|Xinyu Wang et.al.|[2401.06395](http://arxiv.org/abs/2401.06395)|null|\n", "2401.04394": "|**2024-04-03**|**SonicVisionLM: Playing Sound with Vision Language Models**|Zhifeng Xie et.al.|[2401.04394](http://arxiv.org/abs/2401.04394)|null|\n", "2401.01959": "|**2024-01-03**|**Analyzing students collaboratively solving spherical unit vector problems in upper level E and M through a lens of shared resources**|Ying Cao et.al.|[2401.01959](http://arxiv.org/abs/2401.01959)|null|\n", "2401.00424": "|**2023-12-31**|**SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation for Multi-modal Intent Detection**|Shijue Huang et.al.|[2401.00424](http://arxiv.org/abs/2401.00424)|null|\n", "2312.17172": "|**2023-12-28**|**Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action**|Jiasen Lu et.al.|[2312.17172](http://arxiv.org/abs/2312.17172)|null|\n", "2312.14667": "|**2024-06-06**|**Token-Level Contrastive Learning with Modality-Aware Prompting for Multimodal Intent Recognition**|Qianrui Zhou et.al.|[2312.14667](http://arxiv.org/abs/2312.14667)|null|\n", "2312.10579": "|**2024-08-31**|**DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialogue Emotion Recognition**|Wei Ai et.al.|[2312.10579](http://arxiv.org/abs/2312.10579)|null|\n", "2312.05412": "|**2024-10-09**|**CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling**|Ruihan Yang et.al.|[2312.05412](http://arxiv.org/abs/2312.05412)|null|\n", "2311.17177": "|**2023-11-28**|**THInImg: Cross-modal Steganography for Presenting Talking Heads in Images**|Lin Zhao et.al.|[2311.17177](http://arxiv.org/abs/2311.17177)|null|\n", "2311.10170": "|**2023-11-16**|**Improving Unimodal Inference with Multimodal Transformers**|Kateryna Chumachenko et.al.|[2311.10170](http://arxiv.org/abs/2311.10170)|null|\n", "2311.08782": "|**2023-11-15**|**Language Semantic Graph Guided Data-Efficient Learning**|Wenxuan Ma et.al.|[2311.08782](http://arxiv.org/abs/2311.08782)|null|\n", "2311.06217": "|**2024-07-04**|**MultiIoT: Benchmarking Machine Learning for the Internet of Things**|Shentong Mo et.al.|[2311.06217](http://arxiv.org/abs/2311.06217)|null|\n", "2311.05698": "|**2024-04-03**|**Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities**|AJ Piergiovanni et.al.|[2311.05698](http://arxiv.org/abs/2311.05698)|null|\n", "2310.19559": "|**2023-11-02**|**Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning**|Changsheng Lv et.al.|[2310.19559](http://arxiv.org/abs/2310.19559)|null|\n", "2310.18668": "|**2023-10-28**|**FinBTech: Blockchain-Based Video and Voice Authentication System for Enhanced Security in Financial Transactions Utilizing FaceNet512 and Gaussian Mixture Models**|Prof N. Jeenath Laila et.al.|[2310.18668](http://arxiv.org/abs/2310.18668)|null|\n", "2310.16534": "|**2023-10-25**|**An Early Evaluation of GPT-4V(ision)**|Yang Wu et.al.|[2310.16534](http://arxiv.org/abs/2310.16534)|null|\n", "2310.15247": "|**2023-10-23**|**SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis**|Marco Comunit\u00e0 et.al.|[2310.15247](http://arxiv.org/abs/2310.15247)|null|\n", "2310.08487": "|**2023-10-12**|**GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language Models**|Yuanchun Shen et.al.|[2310.08487](http://arxiv.org/abs/2310.08487)|null|\n", "2310.05804": "|**2023-12-14**|**Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis**|Haoyu Zhang et.al.|[2310.05804](http://arxiv.org/abs/2310.05804)|null|\n", "2310.03251": "|**2023-10-05**|**Efficient Video and Audio processing with Loihi 2**|Sumit Bam Shrestha et.al.|[2310.03251](http://arxiv.org/abs/2310.03251)|null|\n", "2309.15136": "|**2024-04-18**|**A multi-modal approach for identifying schizophrenia using cross-modal attention**|Gowtham Premananth et.al.|[2309.15136](http://arxiv.org/abs/2309.15136)|null|\n", "2309.12067": "|**2023-09-21**|**Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives**|Karolina Seweryn et.al.|[2309.12067](http://arxiv.org/abs/2309.12067)|null|\n", "2309.11500": "|**2024-09-09**|**Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning**|Luoyi Sun et.al.|[2309.11500](http://arxiv.org/abs/2309.11500)|null|\n", "2309.10537": "|**2023-09-19**|**FoleyGen: Visually-Guided Audio Generation**|Xinhao Mei et.al.|[2309.10537](http://arxiv.org/abs/2309.10537)|null|\n", "2309.07606": "|**2024-09-10**|**Zero-shot Audio Topic Reranking using Large Language Models**|Mengjie Qian et.al.|[2309.07606](http://arxiv.org/abs/2309.07606)|null|\n", "2309.05519": "|**2024-06-25**|**NExT-GPT: Any-to-Any Multimodal LLM**|Shengqiong Wu et.al.|[2309.05519](http://arxiv.org/abs/2309.05519)|null|\n", "2308.13820": "|**2023-08-26**|**Video and Audio are Images: A Cross-Modal Mixer for Original Data on Video-Audio Retrieval**|Zichen Yuan et.al.|[2308.13820](http://arxiv.org/abs/2308.13820)|null|\n", "2308.12558": "|**2023-12-16**|**Hyperbolic Audio-visual Zero-shot Learning**|Jie Hong et.al.|[2308.12558](http://arxiv.org/abs/2308.12558)|null|\n", "2308.08488": "|**2024-03-09**|**Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder**|Yusheng Dai et.al.|[2308.08488](http://arxiv.org/abs/2308.08488)|null|\n", "2307.16584": "|**2023-07-31**|**Audio-visual video-to-speech synthesis with synthesized input audio**|Triantafyllos Kefalas et.al.|[2307.16584](http://arxiv.org/abs/2307.16584)|null|\n", "2307.16184": "|**2023-12-22**|**UnIVAL: Unified Model for Image, Video, Audio and Language Tasks**|Mustafa Shukor et.al.|[2307.16184](http://arxiv.org/abs/2307.16184)|null|\n", "2307.00610": "|**2023-07-27**|**Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets**|Raphael Frick et.al.|[2307.00610](http://arxiv.org/abs/2307.00610)|null|\n", "2307.07516": "|**2024-03-15**|**Voting-based Multimodal Automatic Deception Detection**|Lana Touma et.al.|[2307.07516](http://arxiv.org/abs/2307.07516)|null|\n", "2306.17404": "|**2023-06-30**|**QuAVF: Quality-aware Audio-Visual Fusion for Ego4D Talking to Me Challenge**|Hsi-Che Lin et.al.|[2306.17404](http://arxiv.org/abs/2306.17404)|null|\n", "2306.17203": "|**2023-06-29**|**Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models**|Simian Luo et.al.|[2306.17203](http://arxiv.org/abs/2306.17203)|null|\n", "2306.16034": "|**2023-06-28**|**Stone Needle: A General Multimodal Large-scale Model Framework towards Healthcare**|Weihua Liu et.al.|[2306.16034](http://arxiv.org/abs/2306.16034)|null|\n", "2306.13968": "|**2023-06-24**|**Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents**|Yash Kumar Atri et.al.|[2306.13968](http://arxiv.org/abs/2306.13968)|null|\n", "2306.12559": "|**2023-06-21**|**Exploring the Role of Audio in Video Captioning**|Yuhan Shen et.al.|[2306.12559](http://arxiv.org/abs/2306.12559)|null|\n", "2307.11519": "|**2023-06-15**|**Multi-modal Hate Speech Detection using Machine Learning**|Fariha Tahosin Boishakhi et.al.|[2307.11519](http://arxiv.org/abs/2307.11519)|null|\n", "2306.04098": "|**2023-06-07**|**Phoenix: A Federated Generative Diffusion Model**|Fiona Victoria Stanley Jothiraj et.al.|[2306.04098](http://arxiv.org/abs/2306.04098)|null|\n", "2305.16103": "|**2023-05-25**|**ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst**|Zijia Zhao et.al.|[2305.16103](http://arxiv.org/abs/2305.16103)|null|\n", "2305.12530": "|**2023-12-08**|**Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio**|Jialu Li et.al.|[2305.12530](http://arxiv.org/abs/2305.12530)|null|\n", "2305.11846": "|**2023-05-19**|**Any-to-Any Generation via Composable Diffusion**|Zineng Tang et.al.|[2305.11846](http://arxiv.org/abs/2305.11846)|null|\n", "2305.08293": "|**2023-05-15**|**Identity-Preserving Talking Face Generation with Landmark and Appearance Priors**|Weizhi Zhong et.al.|[2305.08293](http://arxiv.org/abs/2305.08293)|null|\n", "2305.05534": "|**2023-05-09**|**Integrating Holistic and Local Information to Estimate Emotional Reaction Intensity**|Yini Fang et.al.|[2305.05534](http://arxiv.org/abs/2305.05534)|null|\n", "2305.03907": "|**2024-03-22**|**Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation**|Bolin Lai et.al.|[2305.03907](http://arxiv.org/abs/2305.03907)|null|\n", "2304.05930": "|**2024-09-17**|**MED-VT++: Unifying Multimodal Learning with a Multiscale Encoder-Decoder Video Transformer**|Rezaul Karim et.al.|[2304.05930](http://arxiv.org/abs/2304.05930)|null|\n", "2304.03275": "|**2023-09-18**|**That's What I Said: Fully-Controllable Talking Face Generation**|Youngjoon Jang et.al.|[2304.03275](http://arxiv.org/abs/2304.03275)|null|\n", "2303.16342": "|**2023-09-23**|**Language-Guided Audio-Visual Source Separation via Trimodal Consistency**|Reuben Tan et.al.|[2303.16342](http://arxiv.org/abs/2303.16342)|null|\n", "2303.11403": "|**2023-10-27**|**eP-ALM: Efficient Perceptual Augmentation of Language Models**|Mustafa Shukor et.al.|[2303.11403](http://arxiv.org/abs/2303.11403)|null|\n", "2303.03297": "|**2023-12-06**|**Robust Immersive Telepresence and Mobile Telemanipulation: NimbRo wins ANA Avatar XPRIZE Finals**|Max Schwarz et.al.|[2303.03297](http://arxiv.org/abs/2303.03297)|null|\n", "2302.07702": "|**2023-02-15**|**Audio-Visual Contrastive Learning with Temporal Self-Supervision**|Simon Jenni et.al.|[2302.07702](http://arxiv.org/abs/2302.07702)|null|\n", "2301.09595": "|**2023-02-22**|**Zorro: the masked multimodal transformer**|Adri\u00e0 Recasens et.al.|[2301.09595](http://arxiv.org/abs/2301.09595)|null|\n", "2301.10295": "|**2023-01-04**|**Object Segmentation with Audio Context**|Kaihui Zheng et.al.|[2301.10295](http://arxiv.org/abs/2301.10295)|null|\n", "2212.09478": "|**2023-03-24**|**MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation**|Ludan Ruan et.al.|[2212.09478](http://arxiv.org/abs/2212.09478)|null|\n", "2301.01134": "|**2022-12-15**|**Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in Videos**|Khalid Alnajjar et.al.|[2301.01134](http://arxiv.org/abs/2301.01134)|null|\n", "2212.06246": "|**2023-04-04**|**Jointly Learning Visual and Auditory Speech Representations from Raw Data**|Alexandros Haliassos et.al.|[2212.06246](http://arxiv.org/abs/2212.06246)|null|\n", "2212.05056": "|**2023-05-25**|**Testing Human Ability To Detect Deepfake Images of Human Faces**|Sergi D. Bray et.al.|[2212.05056](http://arxiv.org/abs/2212.05056)|null|\n", "2212.03374": "|**2022-12-07**|**Learning State Transition Rules from Hidden Layers of Restricted Boltzmann Machines**|Koji Watanabe et.al.|[2212.03374](http://arxiv.org/abs/2212.03374)|null|\n", "2212.01040": "|**2022-12-02**|**Role of Audio in Audio-Visual Video Summarization**|Ibrahim Shoer et.al.|[2212.01040](http://arxiv.org/abs/2212.01040)|null|\n", "2211.14667": "|**2022-11-26**|**Deep Fake Detection, Deterrence and Response: Challenges and Opportunities**|Amin Azmoodeh et.al.|[2211.14667](http://arxiv.org/abs/2211.14667)|null|\n", "2211.10624": "|**2023-04-02**|**A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset**|Jiaxin Deng et.al.|[2211.10624](http://arxiv.org/abs/2211.10624)|null|\n", "2211.06783": "|**2022-11-13**|**EdnaML: A Declarative API and Framework for Reproducible Deep Learning**|Abhijit Suprem et.al.|[2211.06783](http://arxiv.org/abs/2211.06783)|null|\n", "2211.06666": "|**2022-11-24**|**Optimizing Bandwidth Sharing for Real-time Traffic in Wireless Networks**|Sushi Anna George et.al.|[2211.06666](http://arxiv.org/abs/2211.06666)|null|\n", "2210.14874": "|**2022-11-04**|**Anisotropic multiresolution analyses for deepfake detection**|Wei Huang et.al.|[2210.14874](http://arxiv.org/abs/2210.14874)|null|\n", "2210.10820": "|**2022-10-19**|**VTC: Improving Video-Text Retrieval with User Comments**|Laura Hanu et.al.|[2210.10820](http://arxiv.org/abs/2210.10820)|null|\n", "2210.09651": "|**2022-10-18**|**Comparison of Popular Video Conferencing Apps Using Client-side Measurements on Different Backhaul Networks**|Rohan Kumar et.al.|[2210.09651](http://arxiv.org/abs/2210.09651)|null|\n", "2210.09461": "|**2023-03-01**|**Token Merging: Your ViT But Faster**|Daniel Bolya et.al.|[2210.09461](http://arxiv.org/abs/2210.09461)|null|\n", "2210.05419": "|**2022-10-11**|**Exploring Interactions and Regulations in Collaborative Learning: An Interdisciplinary Multimodal Dataset**|Yante Li et.al.|[2210.05419](http://arxiv.org/abs/2210.05419)|null|\n", "2210.02755": "|**2022-10-06**|**Audio-Visual Face Reenactment**|Madhav Agarwal et.al.|[2210.02755](http://arxiv.org/abs/2210.02755)|null|\n", "2209.14156": "|**2022-11-02**|**TVLT: Textless Vision-Language Transformer**|Zineng Tang et.al.|[2209.14156](http://arxiv.org/abs/2209.14156)|null|\n", "2210.06943": "|**2022-09-17**|**Understand-Before-Talk (UBT): A Semantic Communication Approach to 6G Networks**|Shiva Raj Pokhrel et.al.|[2210.06943](http://arxiv.org/abs/2210.06943)|null|\n", "2209.04780": "|**2022-09-11**|**MAiVAR: Multimodal Audio-Image and Video Action Recognizer**|Muhammad Bilal Shaikh et.al.|[2209.04780](http://arxiv.org/abs/2209.04780)|null|\n", "2209.04355": "|**2022-09-09**|**MIntRec: A New Dataset for Multimodal Intent Recognition**|Hanlei Zhang et.al.|[2209.04355](http://arxiv.org/abs/2209.04355)|null|\n", "2208.12019": "|**2022-08-25**|**A CNN-LSTM-based hybrid deep learning approach to detect sentiment polarities on Monkeypox tweets**|Krishna Kumar Mohbey et.al.|[2208.12019](http://arxiv.org/abs/2208.12019)|null|\n", "2208.11370": "|**2022-08-24**|**Appraisal of a Random Bit Generator Utilizing Smartphone Sensors as Entropy Source**|Stefan Kutschera et.al.|[2208.11370](http://arxiv.org/abs/2208.11370)|null|\n", "2208.07664": "|**2022-08-16**|**M2HF: Multi-level Multi-modal Hybrid Fusion for Text-Video Retrieval**|Shuo Liu et.al.|[2208.07664](http://arxiv.org/abs/2208.07664)|null|\n", "2207.09956": "|**2022-07-20**|**Telepresence Video Quality Assessment**|Zhenqiang Ying et.al.|[2207.09956](http://arxiv.org/abs/2207.09956)|null|\n", "2207.07972": "|**2022-07-16**|**Certified Neural Network Watermarks with Randomized Smoothing**|Arpit Bansal et.al.|[2207.07972](http://arxiv.org/abs/2207.07972)|null|\n", "2207.03074": "|**2022-07-20**|**Visual-Assisted Sound Source Depth Estimation in the Wild**|Wei Sun et.al.|[2207.03074](http://arxiv.org/abs/2207.03074)|null|\n", "2207.02663": "|**2022-07-06**|**Kaggle Competition: Cantonese Audio-Visual Speech Recognition for In-car Commands**|Wenliang Dai et.al.|[2207.02663](http://arxiv.org/abs/2207.02663)|null|\n", "2204.09273": "|**2022-10-21**|**Sound-Guided Semantic Video Generation**|Seung Hyun Lee et.al.|[2204.09273](http://arxiv.org/abs/2204.09273)|null|\n", "2204.02874": "|**2022-08-02**|**ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound**|Yan-Bo Lin et.al.|[2204.02874](http://arxiv.org/abs/2204.02874)|null|\n", "2203.15183": "|**2022-03-29**|**Visualizations of Complex Sequences of Family-Infant Vocalizations Using Bag-of-Audio-Words Approach Based on Wav2vec 2.0 Features**|Jialu Li et.al.|[2203.15183](http://arxiv.org/abs/2203.15183)|null|\n", "2203.04036": "|**2022-03-17**|**StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN**|Fei Yin et.al.|[2203.04036](http://arxiv.org/abs/2203.04036)|null|\n", "2203.04114": "|**2022-09-01**|**A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification**|Qing Wang et.al.|[2203.04114](http://arxiv.org/abs/2203.04114)|null|\n", "2203.01265": "|**2022-03-02**|**Self-supervised Transformer for Deepfake Detection**|Hanqing Zhao et.al.|[2203.01265](http://arxiv.org/abs/2203.01265)|null|\n", "2202.13433": "|**2022-02-27**|**Feasibility and Acceptability of Remote Neuromotor Rehabilitation Interactions Using Social Robot Augmented Telepresence: A Case Study**|Michael J. Sobrepera et.al.|[2202.13433](http://arxiv.org/abs/2202.13433)|null|\n", "2202.09195": "|**2022-02-18**|**A Review on Methods and Applications in Multimodal Deep Learning**|Jabeen Summaira et.al.|[2202.09195](http://arxiv.org/abs/2202.09195)|null|\n", "2202.00251": "|**2022-02-01**|**A Novel Algorithm In Steganography Using Weighted Matching Technique**|P N Priya et.al.|[2202.00251](http://arxiv.org/abs/2202.00251)|null|\n", "2201.06260": "|**2022-01-17**|**Towards Realistic Visual Dubbing with Heterogeneous Sources**|Tianyi Xie et.al.|[2201.06260](http://arxiv.org/abs/2201.06260)|null|\n", "2201.03804": "|**2022-03-14**|**CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition**|Wenliang Dai et.al.|[2201.03804](http://arxiv.org/abs/2201.03804)|null|\n", "2201.02392": "|**2022-01-07**|**Unwinding Rotations Improves User Comfort with Immersive Telepresence Robots**|Markku Suomalainen et.al.|[2201.02392](http://arxiv.org/abs/2201.02392)|null|\n", "2112.08611": "|**2021-12-16**|**Clickbait in YouTube Prevention, Detection and Analysis of the Bait using Ensemble Learning**|Peya Mowar et.al.|[2112.08611](http://arxiv.org/abs/2112.08611)|null|\n", "2112.04748": "|**2022-09-12**|**LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading**|Leyuan Qu et.al.|[2112.04748](http://arxiv.org/abs/2112.04748)|null|\n", "2112.06646": "|**2024-01-11**|**The Burst Market: the Next Leap for Humanity**|Vincent Yuansang Zha et.al.|[2112.06646](http://arxiv.org/abs/2112.06646)|null|\n", "2112.01368": "|**2021-12-02**|**ScaleVLAD: Improving Multimodal Sentiment Analysis via Multi-Scale Fusion of Locally Descriptors**|Huaishao Luo et.al.|[2112.01368](http://arxiv.org/abs/2112.01368)|null|\n", "2112.00725": "|**2023-01-24**|**The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image**|Yuki M. Asano et.al.|[2112.00725](http://arxiv.org/abs/2112.00725)|null|\n", "2111.12993": "|**2021-11-25**|**PolyViT: Co-training Vision Transformers on Images, Videos and Audio**|Valerii Likhosherstov et.al.|[2111.12993](http://arxiv.org/abs/2111.12993)|null|\n", "2111.09531": "|**2021-11-18**|**Triple Attention Network architecture for MovieQA**|Ankit Shah et.al.|[2111.09531](http://arxiv.org/abs/2111.09531)|null|\n", "2111.01888": "|**2021-11-02**|**A dataset for multi-sensor drone detection**|Fredrik Svanstr\u00f6m et.al.|[2111.01888](http://arxiv.org/abs/2111.01888)|null|\n", "2111.01024": "|**2021-11-01**|**With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition**|Evangelos Kazakos et.al.|[2111.01024](http://arxiv.org/abs/2111.01024)|null|\n", "2110.04440": "|**2021-10-09**|**Multimodal Approach for Assessing Neuromotor Coordination in Schizophrenia Using Convolutional Neural Networks**|Yashish M. Siriwardena et.al.|[2110.04440](http://arxiv.org/abs/2110.04440)|null|\n", "2109.05199": "|**2023-02-13**|**A Survey on Multi-modal Summarization**|Anubhav Jangra et.al.|[2109.05199](http://arxiv.org/abs/2109.05199)|null|\n", "2109.04275": "|**2022-04-02**|**M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining**|Xiao Dong et.al.|[2109.04275](http://arxiv.org/abs/2109.04275)|null|\n", "2109.02993": "|**2021-09-07**|**Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors**|Hasam Khalid et.al.|[2109.02993](http://arxiv.org/abs/2109.02993)|null|\n", "2108.05080": "|**2022-03-01**|**FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset**|Hasam Khalid et.al.|[2108.05080](http://arxiv.org/abs/2108.05080)|null|\n", "2108.04343": "|**2021-08-09**|**Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration**|Siham Yousfi et.al.|[2108.04343](http://arxiv.org/abs/2108.04343)|null|\n", "2108.02147": "|**2021-08-04**|**Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers**|Chiori Hori et.al.|[2108.02147](http://arxiv.org/abs/2108.02147)|null|\n", "2107.03465": "|**2021-08-13**|**An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild**|Panagiotis Antoniadis et.al.|[2107.03465](http://arxiv.org/abs/2107.03465)|null|\n", "2106.09669": "|**2021-10-14**|**Improving On-Screen Sound Separation for Open-Domain Videos with Audio-Visual Self-Attention**|Efthymios Tzinis et.al.|[2106.09669](http://arxiv.org/abs/2106.09669)|null|\n", "2106.05239": "|**2022-04-21**|**XBNet : An Extremely Boosted Neural Network**|Tushar Sarkar et.al.|[2106.05239](http://arxiv.org/abs/2106.05239)|null|\n", "2106.03821": "|**2021-09-15**|**Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion**|Baptiste Pouthier et.al.|[2106.03821](http://arxiv.org/abs/2106.03821)|null|\n", "2106.01667": "|**2021-06-03**|**APES: Audiovisual Person Search in Untrimmed Video**|Juan Leon Alcazar et.al.|[2106.01667](http://arxiv.org/abs/2106.01667)|null|\n", "2106.00451": "|**2021-05-28**|**Highlight Timestamp Detection Model for Comedy Videos via Multimodal Sentiment Analysis**|Fan Huang et.al.|[2106.00451](http://arxiv.org/abs/2106.00451)|null|\n", "2105.11087": "|**2021-05-24**|**Recent Advances and Trends in Multimodal Deep Learning: A Review**|Jabeen Summaira et.al.|[2105.11087](http://arxiv.org/abs/2105.11087)|null|\n", "2105.05542": "|**2021-05-12**|**!Qu\u00e9 maravilla! Multimodal Sarcasm Detection in Spanish: a Dataset and a Baseline**|Khalid Alnajjar et.al.|[2105.05542](http://arxiv.org/abs/2105.05542)|null|\n", "2105.00080": "|**2021-05-24**|**Entangling Quantum Generative Adversarial Networks**|Murphy Yuezhen Niu et.al.|[2105.00080](http://arxiv.org/abs/2105.00080)|null|\n", "2104.01104": "|**2021-05-14**|**Adaptive Bitrate Streaming Over Cellular Networks: Rate Adaptation and Data Savings Strategies**|Yanyuan Qin et.al.|[2104.01104](http://arxiv.org/abs/2104.01104)|null|\n", "2104.02656": "|**2021-04-01**|**Collaborative Learning to Generate Audio-Video Jointly**|Vinod K Kurmi et.al.|[2104.02656](http://arxiv.org/abs/2104.02656)|null|\n", "2103.16559": "|**2021-10-19**|**Broaden Your Views for Self-Supervised Video Learning**|Adri\u00e0 Recasens et.al.|[2103.16559](http://arxiv.org/abs/2103.16559)|null|\n", "2103.14224": "|**2021-07-25**|**Active multi-fidelity Bayesian online changepoint detection**|Gregory W. Gundersen et.al.|[2103.14224](http://arxiv.org/abs/2103.14224)|null|\n", "2104.02775": "|**2021-03-25**|**Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation**|Jiyoung Lee et.al.|[2104.02775](http://arxiv.org/abs/2104.02775)|null|\n", "2103.03700": "|**2021-03-05**|**Analyzing the Influence of Dataset Composition for Emotion Recognition**|A. Sutherland et.al.|[2103.03700](http://arxiv.org/abs/2103.03700)|null|\n", "2103.02032": "|**2021-03-02**|**Not engaging with problems in the lab: Students' navigation of conflicting data and models**|Anna McLean Phillips et.al.|[2103.02032](http://arxiv.org/abs/2103.02032)|null|\n", "2102.05950": "|**2021-02-11**|**Adversarially robust deepfake media detection using fused convolutional neural network predictions**|Sohail Ahmed Khan et.al.|[2102.05950](http://arxiv.org/abs/2102.05950)|null|\n", "2101.12059": "|**2021-01-29**|**VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs**|Xudong Lin et.al.|[2101.12059](http://arxiv.org/abs/2101.12059)|null|\n", "2101.08846": "|**2021-01-21**|**Soloist: Generating Mixed-Initiative Tutorials from Existing Guitar Instructional Videos Through Audio Processing**|Bryan Wang et.al.|[2101.08846](http://arxiv.org/abs/2101.08846)|null|\n", "2102.08941": "|**2021-01-10**|**Automatic Face Understanding: Recognizing Families in Photos**|Joseph P Robinson et.al.|[2102.08941](http://arxiv.org/abs/2102.08941)|null|\n", "2012.03091": "|**2021-07-06**|**Over a Decade of Social Opinion Mining: A Systematic Review**|Keith Cortis et.al.|[2012.03091](http://arxiv.org/abs/2012.03091)|null|\n", "2011.09804": "|**2020-11-19**|**TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos**|Manuel Sam Ribeiro et.al.|[2011.09804](http://arxiv.org/abs/2011.09804)|null|\n", "2011.07355": "|**2020-11-14**|**Towards transformation-resilient provenance detection of digital media**|Jamie Hayes et.al.|[2011.07355](http://arxiv.org/abs/2011.07355)|null|\n", "2011.05698": "|**2020-11-11**|**On Performance of Multiscale Sparse Fast Fourier Transform Algorithm**|Bin Li et.al.|[2011.05698](http://arxiv.org/abs/2011.05698)|null|\n", "2011.01819": "|**2020-11-03**|**Learning Representations from Audio-Visual Spatial Alignment**|Pedro Morgado et.al.|[2011.01819](http://arxiv.org/abs/2011.01819)|null|\n", "2011.01143": "|**2021-05-30**|**Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds**|Efthymios Tzinis et.al.|[2011.01143](http://arxiv.org/abs/2011.01143)|null|\n", "2008.13269": "|**2020-08-30**|**Joint Transmission in QoE-Driven Backhaul-Aware MC-NOMA Cognitive Radio Network**|Hosein Zarini et.al.|[2008.13269](http://arxiv.org/abs/2008.13269)|null|\n", "2008.09586": "|**2021-03-12**|**An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation**|Daniel Michelsanti et.al.|[2008.09586](http://arxiv.org/abs/2008.09586)|null|\n", "2008.06607": "|**2020-08-14**|**Self-supervised Contrastive Video-Speech Representation Learning for Ultrasound**|Jianbo Jiao et.al.|[2008.06607](http://arxiv.org/abs/2008.06607)|null|\n", "2008.05691": "|**2020-10-20**|**Social App Accessibility for Deaf Signers**|Kelly Mack et.al.|[2008.05691](http://arxiv.org/abs/2008.05691)|null|\n", "2007.10984": "|**2020-07-21**|**Foley Music: Learning to Generate Music from Videos**|Chuang Gan et.al.|[2007.10984](http://arxiv.org/abs/2007.10984)|null|\n", "2007.07396": "|**2020-10-19**|**Real-Time Drone Detection and Tracking With Visible, Thermal and Acoustic Sensors**|Fredrik Svanstrom et.al.|[2007.07396](http://arxiv.org/abs/2007.07396)|null|\n", "2007.04687": "|**2020-07-13**|**Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision**|Peng Wu et.al.|[2007.04687](http://arxiv.org/abs/2007.04687)|null|\n", "2006.16228": "|**2020-10-30**|**Self-Supervised MultiModal Versatile Networks**|Jean-Baptiste Alayrac et.al.|[2006.16228](http://arxiv.org/abs/2006.16228)|null|\n", "2006.14348": "|**2020-06-23**|**Audeo: Audio Generation for a Silent Performance Video**|Kun Su et.al.|[2006.14348](http://arxiv.org/abs/2006.14348)|null|\n", "2005.13362": "|**2020-05-28**|**A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews**|Edison Marrese-Taylor et.al.|[2005.13362](http://arxiv.org/abs/2005.13362)|null|\n", "2005.11437": "|**2020-05-23**|**S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation**|Yizhe Zhu et.al.|[2005.11437](http://arxiv.org/abs/2005.11437)|null|\n", "2004.12943": "|**2021-03-29**|**Audio-Visual Instance Discrimination with Cross-Modal Agreement**|Pedro Morgado et.al.|[2004.12943](http://arxiv.org/abs/2004.12943)|null|\n", "2004.02113": "|**2020-04-05**|**Emotional Video to Audio Transformation Using Deep Recurrent Neural Networks and a Neuro-Fuzzy System**|Gwenaelle Cunha Sergio et.al.|[2004.02113](http://arxiv.org/abs/2004.02113)|null|\n", "2003.09234": "|**2020-03-11**|**DeepFake Detection: Current Challenges and Next Steps**|Siwei Lyu et.al.|[2003.09234](http://arxiv.org/abs/2003.09234)|null|\n", "2002.03844": "|**2020-06-06**|**Exploiting Temporal Coherence for Multi-modal Video Categorization**|Palash Goyal et.al.|[2002.03844](http://arxiv.org/abs/2002.03844)|null|\n", "1911.12667": "|**2020-10-26**|**Self-Supervised Learning by Cross-Modal Audio-Video Clustering**|Humam Alwassel et.al.|[1911.12667](http://arxiv.org/abs/1911.12667)|null|\n", "1911.06476": "|**2019-11-15**|**Deep Long Audio Inpainting**|Ya-Liang Chang et.al.|[1911.06476](http://arxiv.org/abs/1911.06476)|null|\n", "1910.13148": "|**2019-10-29**|**A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models**|Maksim Kuznetsov et.al.|[1910.13148](http://arxiv.org/abs/1910.13148)|null|\n", "1909.06957": "|**2019-09-17**|**Multimodal Deep Models for Predicting Affective Responses Evoked by Movies**|Ha Thi Phuong Thao et.al.|[1909.06957](http://arxiv.org/abs/1909.06957)|null|\n", "1908.10834": "|**2020-09-11**|**AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing**|Tong Geng et.al.|[1908.10834](http://arxiv.org/abs/1908.10834)|null|\n", "1907.04476": "|**2019-10-31**|**A New Benchmark and Approach for Fine-grained Cross-media Retrieval**|Xiangteng He et.al.|[1907.04476](http://arxiv.org/abs/1907.04476)|null|\n", "1907.04474": "|**2019-07-10**|**Ultrareliable and Low-Latency Communication Techniques for Tactile Internet Services**|Kwang Soon Kim et.al.|[1907.04474](http://arxiv.org/abs/1907.04474)|null|\n", "1907.04926": "|**2019-07-05**|**Synchronizing Audio-Visual Film Stimuli in Unity (version 5.5.1f1): Game Engines as a Tool for Research**|Javier Sanz et.al.|[1907.04926](http://arxiv.org/abs/1907.04926)|null|\n", "1906.07901": "|**2019-06-19**|**Multimodal Abstractive Summarization for How2 Videos**|Shruti Palaskar et.al.|[1906.07901](http://arxiv.org/abs/1906.07901)|null|\n", "1904.07933": "|**2020-02-11**|**Audio-Visual Model Distillation Using Acoustic Images**|Andr\u00e9s F. P\u00e9rez et.al.|[1904.07933](http://arxiv.org/abs/1904.07933)|null|\n", "1904.07750": "|**2019-08-20**|**Co-Separating Sounds of Visual Objects**|Ruohan Gao et.al.|[1904.07750](http://arxiv.org/abs/1904.07750)|null|\n", "1904.04446": "|**2019-04-09**|**HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition**|Wenxiang Jiao et.al.|[1904.04446](http://arxiv.org/abs/1904.04446)|null|\n", "1901.09107": "|**2019-05-08**|**Audio-Visual Scene-Aware Dialog**|Huda Alamri et.al.|[1901.09107](http://arxiv.org/abs/1901.09107)|null|\n", "1901.08759": "|**2019-01-25**|**Misleading Metadata Detection on YouTube**|Priyank Palod et.al.|[1901.08759](http://arxiv.org/abs/1901.08759)|null|\n", "1901.04110": "|**2019-01-14**|**Machine learning for the recognition of emotion in the speech of couples in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset**|Colleen E. Crangle et.al.|[1901.04110](http://arxiv.org/abs/1901.04110)|null|\n", "1812.07023": "|**2018-12-17**|**From FiLM to Video: Multi-turn Question Answering with Multi-modal Context**|Dat Tien Nguyen et.al.|[1812.07023](http://arxiv.org/abs/1812.07023)|null|\n", "1810.00530": "|**2018-10-01**|**Learnable Pooling Methods for Video Classification**|Sebastian Kmiec et.al.|[1810.00530](http://arxiv.org/abs/1810.00530)|null|\n", "1809.03445": "|**2018-09-10**|**A collection of database industrial techniques and optimization approaches of database operations**|Jasper Kyle Catapang et.al.|[1809.03445](http://arxiv.org/abs/1809.03445)|null|\n", "1807.01122": "|**2018-07-03**|**Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities**|Nathaniel Blanchard et.al.|[1807.01122](http://arxiv.org/abs/1807.01122)|null|\n", "1807.00612": "|**2020-04-30**|**Multi-modal Egocentric Activity Recognition using Audio-Visual Features**|Mehmet Ali Arabac\u0131 et.al.|[1807.00612](http://arxiv.org/abs/1807.00612)|null|\n", "1806.03497": "|**2018-06-09**|**Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction**|Siyuan Qi et.al.|[1806.03497](http://arxiv.org/abs/1806.03497)|null|\n", "1806.00737": "|**2018-06-03**|**Content-based Video Relevance Prediction Challenge: Data, Protocol, and Baseline**|Mengyi Liu et.al.|[1806.00737](http://arxiv.org/abs/1806.00737)|null|\n", "1805.11254": "|**2018-08-15**|**Hierarchical One Permutation Hashing: Efficient Multimedia Near Duplicate Detection**|Chengyuan Zhang et.al.|[1805.11254](http://arxiv.org/abs/1805.11254)|null|\n", "1805.10030": "|**2019-09-08**|**DIF : Dataset of Perceived Intoxicated Faces for Drunk Person Identification**|Vineet Mehta et.al.|[1805.10030](http://arxiv.org/abs/1805.10030)|null|\n", "1804.03641": "|**2018-10-09**|**Audio-Visual Scene Analysis with Self-Supervised Multisensory Features**|Andrew Owens et.al.|[1804.03641](http://arxiv.org/abs/1804.03641)|null|\n", "1803.02991": "|**2018-06-12**|**Disentangled Sequential Autoencoder**|Yingzhen Li et.al.|[1803.02991](http://arxiv.org/abs/1803.02991)|null|\n", "1802.06852": "|**2018-02-19**|**Be-Educated: Multimedia Learning through 3D Animation**|Zeeshan Bhatti et.al.|[1802.06852](http://arxiv.org/abs/1802.06852)|null|\n", "1801.10123": "|**2018-01-30**|**Links: A High-Dimensional Online Clustering Method**|Philip Andrew Mansfield et.al.|[1801.10123](http://arxiv.org/abs/1801.10123)|null|\n", "1801.05076": "|**2018-01-16**|**Analytic Provenance Datasets: A Data Repository of Human Analysis Activity and Interaction Logs**|Sina Mohseni et.al.|[1801.05076](http://arxiv.org/abs/1801.05076)|null|\n", "1801.02200": "|**2018-01-07**|**Cross-modal Embeddings for Video and Audio Retrieval**|Didac Sur\u00eds et.al.|[1801.02200](http://arxiv.org/abs/1801.02200)|null|\n", "1711.11118": "|**2017-11-29**|**Multimodal Attribute Extraction**|Robert L. Logan et.al.|[1711.11118](http://arxiv.org/abs/1711.11118)|null|\n", "1711.06550": "|**2017-11-14**|**Reconstruction of the External Stimuli from Brain Signals**|Pouya Ghaemmaghami et.al.|[1711.06550](http://arxiv.org/abs/1711.06550)|null|\n", "1709.07200": "|**2017-09-21**|**Temporal Multimodal Fusion for Video Emotion Classification in the Wild**|Valentin Vielzeuf et.al.|[1709.07200](http://arxiv.org/abs/1709.07200)|null|\n", "1706.05461": "|**2017-07-10**|**Truly Multi-modal YouTube-8M Video Classification with Video, Audio, and Text**|Zhe Wang et.al.|[1706.05461](http://arxiv.org/abs/1706.05461)|null|\n", "1705.03744": "|**2017-04-20**|**Signal Classification in Quotient Spaces via Globally Optimal Variational Calculus**|Gregory S Chirikjian et.al.|[1705.03744](http://arxiv.org/abs/1705.03744)|null|\n", "1704.00063": "|**2017-03-31**|**TopologyNet: Topology based deep convolutional neural networks for biomolecular property predictions**|Zixuan Cang et.al.|[1704.00063](http://arxiv.org/abs/1704.00063)|null|\n", "1703.01925": "|**2017-03-06**|**Grammar Variational Autoencoder**|Matt J. Kusner et.al.|[1703.01925](http://arxiv.org/abs/1703.01925)|null|\n", "1703.01725": "|**2017-03-06**|**Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity**|Jack Hessel et.al.|[1703.01725](http://arxiv.org/abs/1703.01725)|null|\n", "1702.08539": "|**2017-02-27**|**Non-Concave Network Utility Maximization in Connectionless Networks: A Fully Distributed Traffic Allocation Algorithm**|Jingyao Wang et.al.|[1702.08539](http://arxiv.org/abs/1702.08539)|null|\n", "1702.06151": "|**2017-02-20**|**Developing a comprehensive framework for multimodal feature extraction**|Quinten McNamara et.al.|[1702.06151](http://arxiv.org/abs/1702.06151)|null|\n", "1701.04224": "|**2017-03-17**|**Auxiliary Multimodal LSTM for Audio-visual Speech Recognition and Lipreading**|Chunlin Tian et.al.|[1701.04224](http://arxiv.org/abs/1701.04224)|null|\n", "1701.05088": "|**2017-01-09**|**Temporal scale selection in time-causal scale space**|Tony Lindeberg et.al.|[1701.05088](http://arxiv.org/abs/1701.05088)|null|\n", "1610.03577": "|**2017-12-01**|**Minimax Filter: Learning to Preserve Privacy from Inference Attacks**|Jihun Hamm et.al.|[1610.03577](http://arxiv.org/abs/1610.03577)|null|\n", "1608.05613": "|**2016-08-18**|**A short-key one-time pad cipher**|Uwe Starossek et.al.|[1608.05613](http://arxiv.org/abs/1608.05613)|null|\n", "1602.08750": "|**2016-02-28**|**Filtering Video Noise as Audio with Motion Detection to Form a Musical Instrument**|Carl Thom\u00e9 et.al.|[1602.08750](http://arxiv.org/abs/1602.08750)|null|\n", "1511.08260": "|**2015-12-08**|**Unsupervised decoding of long-term, naturalistic human neural recordings with automated video and audio annotations**|Nancy X. R. Wang et.al.|[1511.08260](http://arxiv.org/abs/1511.08260)|null|\n", "1510.04221": "|**2015-10-14**|**Automatic Stress Detection in Working Environments from Smartphones' Accelerometer Data: A First Step**|Enrique Garcia-Ceja et.al.|[1510.04221](http://arxiv.org/abs/1510.04221)|null|\n", "1501.04155": "|**2015-01-17**|**Real Time Collaborative Platform for Learning and Teaching Foreign Languages**|Ilya V. Osipov et.al.|[1501.04155](http://arxiv.org/abs/1501.04155)|null|\n", "1411.6496": "|**2014-11-24**|**Automatic Summarization of Soccer Highlights Using Audio-visual Descriptors**|Arnau Raventos et.al.|[1411.6496](http://arxiv.org/abs/1411.6496)|null|\n", "1403.5912": "|**2014-03-24**|**The state of play of ASC-Inclusion: An Integrated Internet-Based Environment for Social Inclusion of Children with Autism Spectrum Conditions**|Bj\u00f6rn Schuller et.al.|[1403.5912](http://arxiv.org/abs/1403.5912)|null|\n", "1310.1551": "|**2013-10-06**|**Optical Disk with Blu-Ray Technology**|T. Ravi Kumar et.al.|[1310.1551](http://arxiv.org/abs/1310.1551)|null|\n", "1209.3644": "|**2012-09-17**|**Availability of titles on peer-to-peer file sharing networks**|Petrus H. Potgieter et.al.|[1209.3644](http://arxiv.org/abs/1209.3644)|null|\n", "1209.2070": "|**2012-08-15**|**Content-based Multi-media Retrieval Technology**|Yi Wang et.al.|[1209.2070](http://arxiv.org/abs/1209.2070)|null|\n", "1203.2299": "|**2012-03-11**|**A Cross-cultural Corpus of Annotated Verbal and Nonverbal Behaviors in Receptionist Encounters**|Maxim Makatchev et.al.|[1203.2299](http://arxiv.org/abs/1203.2299)|null|\n", "1111.1817": "|**2014-05-14**|**Hierarchical Hidden Markov Model in Detecting Activities of Daily Living in Wearable Videos for Studies of Dementia**|Svebor Karaman et.al.|[1111.1817](http://arxiv.org/abs/1111.1817)|null|\n", "1007.1473": "|**2010-07-08**|**Intrusions into Privacy in Video Chat Environments: Attacks and Countermeasures**|Xinyu Xing et.al.|[1007.1473](http://arxiv.org/abs/1007.1473)|null|\n", "1003.3533": "|**2010-03-18**|**Towards Automated Lecture Capture, Navigation and Delivery System for Web-Lecture on Demand**|Rajkumar Kannan et.al.|[1003.3533](http://arxiv.org/abs/1003.3533)|null|\n"}}